{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RL coursework, part I (20 pts total)\n",
    "---\n",
    "\n",
    "\n",
    "**Name:** Yuan Lu\n",
    "\n",
    "**SN:** 20114649\n",
    "\n",
    "---\n",
    "\n",
    "**Due date:** *April 10th, 2025*\n",
    "\n",
    "---\n",
    "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
    "\n",
    "\n",
    "## How to submit\n",
    "\n",
    "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part1.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "**Reminder of copyrights**\n",
    "\n",
    "Copyrights protect this code/content and distribution or usages of it (or parts of it) without permission is prohibited. This includes uploading it and usage of it in training in any LLMs systems."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Context**\n",
    "\n",
    "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
    "\n",
    "**Background reading**\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 1 to 6\n",
    "* Lecture slides"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Overview of this assignment**\n",
    "\n",
    "A) You will use Python to implement several bandit algorithms.\n",
    "\n",
    "B) You will then run these algorithms on a multi-armed Bernoulli bandit problem, and answer question about their empirical performance.\n",
    "\n",
    "C) You will then be asked to reason about the behaviour of different algorithms"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import Useful Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns  # Import Seaborn\n",
    "from functools import partial\n",
    "import collections\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BernoulliBandit(object):\n",
    "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
    "\n",
    "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
    "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
    "\n",
    "    Args:\n",
    "    success_probabilities: A list or numpy array containing the probabilities,\n",
    "    for each of the arms, of providing a success reward.\n",
    "    success_reward: The reward on success (default: 1.)\n",
    "    fail_reward: The reward on failure (default: 0.)\n",
    "    \"\"\"\n",
    "    self._probs = success_probabilities\n",
    "    self._number_of_arms = len(self._probs)\n",
    "    self._s = success_reward\n",
    "    self._f = fail_reward\n",
    "\n",
    "    ps = np.array(success_probabilities)\n",
    "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"The step function.\n",
    "\n",
    "    Args:\n",
    "      action: An integer or np.int32 that specifies which arm to pull.\n",
    "\n",
    "    Returns:\n",
    "      A reward sampled according to the success probability of the selected arm.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: when the provided action is out of bounds.\n",
    "    \"\"\"\n",
    "    if action < 0 or action >= self._number_of_arms:\n",
    "      raise ValueError('Action {} is out of bounds for a '\n",
    "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
    "\n",
    "    success = bool(np.random.random() < self._probs[action])\n",
    "    reward = success * self._s + (not success) * self._f\n",
    "    return reward\n",
    "\n",
    "  def regret(self, action):\n",
    "    \"\"\"Computes the regret for the given action.\"\"\"\n",
    "    return self._values.max() - self._values[action]\n",
    "\n",
    "  def optimal_value(self):\n",
    "    \"\"\"Computes the regret for the given action.\"\"\"\n",
    "    return self._values.max()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NonStationaryBandit(object):\n",
    "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
    "\n",
    "  def __init__(self, success_probabilities,\n",
    "               success_reward=1., fail_reward=0., change_point=800,\n",
    "               change_is_good=True):\n",
    "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
    "\n",
    "    Args:\n",
    "      success_probabilities: A list or numpy array containing the probabilities,\n",
    "          for each of the arms, of providing a success reward.\n",
    "      success_reward: The reward on success (default: 1.)\n",
    "      fail_reward: The reward on failure (default: 0.)\n",
    "      change_point: The number of steps before the rewards change.\n",
    "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
    "    \"\"\"\n",
    "    self._probs = success_probabilities\n",
    "    self._number_of_arms = len(self._probs)\n",
    "    self._s = success_reward\n",
    "    self._f = fail_reward\n",
    "    self._change_point = change_point\n",
    "    self._change_is_good = change_is_good\n",
    "    self._number_of_steps_so_far = 0\n",
    "\n",
    "    ps = np.array(success_probabilities)\n",
    "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"The step function.\n",
    "\n",
    "    Args:\n",
    "      action: An integer or np.int32 that specifies which arm to pull.\n",
    "\n",
    "    Returns:\n",
    "      A reward sampled according to the success probability of the selected arm.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: when the provided action is out of bounds.\n",
    "    \"\"\"\n",
    "    if action < 0 or action >= self._number_of_arms:\n",
    "      raise ValueError('Action {} is out of bounds for a '\n",
    "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
    "\n",
    "    self._number_of_steps_so_far += 1\n",
    "    success = bool(np.random.random() < self._probs[action])\n",
    "    reward = success * self._s + (not success) * self._f\n",
    "\n",
    "    if self._number_of_steps_so_far == self._change_point:\n",
    "      # After some number of steps, the rewards are inverted\n",
    "      #\n",
    "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
    "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
    "      #            - 1984, Orwell (1949).\n",
    "      reward_dif = (self._s - self._f)\n",
    "      if self._change_is_good:\n",
    "        self._f = self._s + reward_dif\n",
    "      else:\n",
    "        self._s -= reward_dif\n",
    "        self._f += reward_dif\n",
    "\n",
    "      # Recompute expected values when the rewards change\n",
    "      ps = np.array(self._probs)\n",
    "      self._values = ps * self._s + (1 - ps) * self._f\n",
    "\n",
    "    return reward\n",
    "\n",
    "  def regret(self, action):\n",
    "    \"\"\"Computes the regret for the given action.\"\"\"\n",
    "    return self._values.max() - self._values[action]\n",
    "\n",
    "  def optimal_value(self):\n",
    "    \"\"\"Computes the regret for the given action.\"\"\"\n",
    "    return self._values.max()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Helper functions\n",
    "\n",
    "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
    "  \"\"\"Smoothing function for plotting.\"\"\"\n",
    "  smoothed_array = []\n",
    "  value = initial_value\n",
    "  b = 1./smoothing_horizon\n",
    "  m = 1.\n",
    "  for x in array:\n",
    "    m *= 1. - b\n",
    "    lr = b/(1 - m)\n",
    "    value += lr*(x - value)\n",
    "    smoothed_array.append(value)\n",
    "  return np.array(smoothed_array)\n",
    "\n",
    "def plot(algs, plot_data, repetitions=30):\n",
    "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
    "  algs_per_row = 4\n",
    "  n_algs = len(algs)\n",
    "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
    "  fig = plt.figure(figsize=(10, 4*n_rows))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
    "  clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
    "  lss = ['--', '-', '-', '-', '-']\n",
    "  for i, p in enumerate(plot_data):\n",
    "    for c in range(n_rows):\n",
    "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
    "      ax.grid(0)\n",
    "\n",
    "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
    "      for alg, clr, ls in zip(current_algs, clrs, lss):\n",
    "        data = p.data[alg.name]\n",
    "        m = smooth(np.mean(data, axis=0))\n",
    "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
    "        if p.log_plot:\n",
    "          line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
    "                              color=clr, ls=ls, lw=3)[0]\n",
    "        else:\n",
    "          line = plt.plot(m, alpha=0.7, label=alg.name,\n",
    "                          color=clr, ls=ls, lw=3)[0]\n",
    "          plt.fill_between(range(len(m)), m + s, m - s,\n",
    "                           color=line.get_color(), alpha=0.2)\n",
    "      if p.opt_values is not None:\n",
    "        plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
    "                 label='optimal')\n",
    "\n",
    "      ax.set_facecolor('white')\n",
    "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
    "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "      ax.spines[\"top\"].set_visible(False)\n",
    "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
    "      ax.spines[\"right\"].set_visible(False)\n",
    "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
    "      ax.get_xaxis().tick_bottom()\n",
    "      ax.get_yaxis().tick_left()\n",
    "\n",
    "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
    "\n",
    "      if p.log_plot:\n",
    "        start, end = calculate_lims(data, p.log_plot)\n",
    "        start = np.floor(np.log10(start))\n",
    "        end = np.ceil(np.log10(end))\n",
    "        ticks = [_*10**__\n",
    "                 for _ in [1., 2., 3., 5.]\n",
    "                 for __ in [-2., -1., 0.]]\n",
    "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
    "                  for _ in [1, 2, 3, 5]\n",
    "                  for __ in [-2, -1, 0]]\n",
    "        plt.yticks(ticks, labels)\n",
    "      plt.ylim(calculate_lims(data, p.log_plot))\n",
    "      plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "      plt.title(p.title)\n",
    "      if i == len(plot_data) - 1:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
    "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
    "  reward_dict = {}\n",
    "  regret_dict = {}\n",
    "  optimal_value_dict = {}\n",
    "\n",
    "  for alg in algs:\n",
    "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
    "\n",
    "    for _rep in range(repetitions):\n",
    "      bandit = bandit_constructor()\n",
    "      alg.reset()\n",
    "\n",
    "      action = None\n",
    "      reward = None\n",
    "      for _step in range(number_of_steps):\n",
    "        action = alg.step(action, reward)\n",
    "        reward = bandit.step(action)\n",
    "        regret = bandit.regret(action)\n",
    "        optimal_value = bandit.optimal_value()\n",
    "\n",
    "        reward_dict[alg.name][_rep, _step] = reward\n",
    "        regret_dict[alg.name][_rep, _step] = regret\n",
    "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
    "\n",
    "  return reward_dict, regret_dict, optimal_value_dict\n",
    "\n",
    "\n",
    "def train_agents(agents, number_of_arms, number_of_steps, repetitions=100,\n",
    "                 success_reward=1., fail_reward=0.,\n",
    "                 bandit_class=BernoulliBandit):\n",
    "\n",
    "  success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
    "\n",
    "  bandit_constructor = partial(bandit_class,\n",
    "                               success_probabilities=success_probabilities,\n",
    "                               success_reward=success_reward,\n",
    "                               fail_reward=fail_reward)\n",
    "  rewards, regrets, opt_values = run_experiment(\n",
    "      bandit_constructor, agents, repetitions, number_of_steps)\n",
    "\n",
    "  smoothed_rewards = {}\n",
    "  for agent, rs in rewards.items():\n",
    "    smoothed_rewards[agent] = np.array(rs)\n",
    "\n",
    "  PlotData = collections.namedtuple('PlotData',\n",
    "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
    "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
    "  plot_data = [\n",
    "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
    "               opt_values=opt_values, log_plot=False),\n",
    "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
    "               log_plot=True),\n",
    "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
    "               log_plot=False),\n",
    "  ]\n",
    "\n",
    "  plot(agents, plot_data, repetitions)\n",
    "\n",
    "def calculate_lims(data, log_plot=False):\n",
    "  y_min = np.min(data)\n",
    "  y_max = np.max(data)\n",
    "  diff = y_max - y_min\n",
    "  if log_plot:\n",
    "    y_min = 0.9*y_min\n",
    "    y_max = 1.1*y_max\n",
    "  else:\n",
    "    y_min = y_min - 0.05*diff\n",
    "    y_max = y_max + 0.05*diff\n",
    "  return y_min, y_max\n",
    "\n",
    "def argmax(array):\n",
    "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
    "  return np.random.choice(np.flatnonzero(array == array.max()))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A) Agent implementations"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "All agents should be in pure Python/NumPy.\n",
    "\n",
    "You cannot use any AutoDiff packages (Jax, TF, PyTorch, etc.)\n",
    "\n",
    "Each agent, should implement the following methods:\n",
    "\n",
    "**`step(self, previous_action, reward)`:**\n",
    "\n",
    "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
    "\n",
    "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
    "\n",
    "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
    "\n",
    "**`reset(self)`:**\n",
    "\n",
    "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
    "\n",
    "Make sure that the initial values (after a reset) are all zero.\n",
    "\n",
    "**`__init__(self, name, number_of_arms, *args)`:**\n",
    "\n",
    "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example agent\n",
    "\n",
    "The following code block contains an example random agent."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "\n",
    "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
    "  random. The 'previous_action' argument of 'step' is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name, number_of_arms):\n",
    "    \"\"\"Initialise the agent.\n",
    "\n",
    "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
    "    bandits `arm` is just another word for `action`.)\n",
    "    \"\"\"\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = name\n",
    "\n",
    "  def step(self, unused_previous_action, unused_reward):\n",
    "    \"\"\"Returns a random action.\n",
    "\n",
    "    The inputs are ignored, but this function still requires an action and a\n",
    "    reward, to have the same interface as other agents who may use these inputs\n",
    "    to learn.\n",
    "    \"\"\"\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "\n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Q1 [2 pts]\n",
    "Implement a UCB agent.\n",
    "\n",
    "The `bonus_multiplier` is the parameter $c$ from the slides."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UCB(object):\n",
    "  def __init__(self, name, number_of_arms, bonus_multiplier):\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._bonus_multiplier = bonus_multiplier\n",
    "    self.name = name\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    # If an action was previously taken, update its statistics.\n",
    "    if previous_action is not None:\n",
    "        self._N[previous_action] += 1\n",
    "        # Incremental update of estimated value for the arm.\n",
    "        self._Q[previous_action] += (reward - self._Q[previous_action]) / self._N[previous_action]\n",
    "\n",
    "    # Increase the time counter (number of steps taken so far)\n",
    "    self._time += 1\n",
    "\n",
    "    # Compute UCB values for each arm\n",
    "    ucb_values = np.zeros(self._number_of_arms)\n",
    "    for i in range(self._number_of_arms):\n",
    "        if self._N[i] == 0:\n",
    "            # Ensure each arm is pulled at least once\n",
    "            ucb_values[i] = float('inf')\n",
    "        else:\n",
    "            bonus = self._bonus_multiplier * np.sqrt(np.log(self._time) / self._N[i])\n",
    "            ucb_values[i] = self._Q[i] + bonus\n",
    "\n",
    "    # Choose the arm with the highest UCB value (break ties randomly)\n",
    "    action = np.random.choice(np.flatnonzero(ucb_values == ucb_values.max()))\n",
    "    return action\n",
    "\n",
    "  def reset(self):\n",
    "    # Reset estimates and counts; initial estimates are all 0.\n",
    "    self._Q = np.zeros(self._number_of_arms)\n",
    "    self._N = np.zeros(self._number_of_arms)\n",
    "    self._time = 0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q2 [1 pt]\n",
    "Implement an $\\epsilon$-greedy agent.\n",
    "\n",
    "This agent should be able to support time-changing $\\epsilon$ schedules.\n",
    "\n",
    "Thus, your agent should accept both constants and callables as constructor argument `epsilon`; callables are used to decay the $\\epsilon$ parameter over time, for instance according to a polynomial schedule: $\\epsilon_t = t^{-\\eta}$ with $\\eta \\in [0, 1]$).\n",
    "\n",
    "\n",
    "If multiple actions have the same value, ties should be broken randomly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EpsilonGreedy(object):\n",
    "  \"\"\"An epsilon-greedy agent.\n",
    "\n",
    "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
    "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
    "  with probability `epsilon` it samples an action uniformly at random.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name, number_of_arms, epsilon=0.1):\n",
    "    self.name = name\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self._epsilon = epsilon  # Can be a constant or a callable schedule.\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    \"\"\"Update the learnt statistics and return an action.\n",
    "\n",
    "    A single call to step uses the provided reward to update the value of the\n",
    "    taken action (which is also provided as an input), and returns an action.\n",
    "    The action is either uniformly random (with probability epsilon), or greedy\n",
    "    (with probability 1 - epsilon).\n",
    "\n",
    "    If the input action is None (typically on the first call to step), then no\n",
    "    statistics are updated, but an action is still returned.\n",
    "    \"\"\"\n",
    "    # If an action was taken previously, update its statistics.\n",
    "    if previous_action is not None:\n",
    "        self._N[previous_action] += 1\n",
    "        self._Q[previous_action] += (reward - self._Q[previous_action]) / self._N[previous_action]\n",
    "\n",
    "    self._time += 1\n",
    "\n",
    "    # Determine the current epsilon value: evaluate if callable, else use constant.\n",
    "    if callable(self._epsilon):\n",
    "        current_epsilon = self._epsilon(self._time)\n",
    "    else:\n",
    "        current_epsilon = self._epsilon\n",
    "\n",
    "    # With probability ε, choose a random action (exploration)\n",
    "    if np.random.random() < current_epsilon:\n",
    "        action = np.random.randint(self._number_of_arms)\n",
    "    else:\n",
    "        # Otherwise, choose the best action (exploitation). Ties are broken randomly.\n",
    "        action = np.random.choice(np.flatnonzero(self._Q == self._Q.max()))\n",
    "    return action\n",
    "\n",
    "  def reset(self):\n",
    "    # Reset estimated values and counts. Initial estimates are all 0.\n",
    "    self._Q = np.zeros(self._number_of_arms)\n",
    "    self._N = np.zeros(self._number_of_arms)\n",
    "    self._time = 0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q3 [2 pts]\n",
    "Implement a REINFORCE agent.\n",
    "\n",
    "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice. In this exercise we ask you to implement REINFORCE with the `square-max` policy parameterization. With this parametrisation the probabilities depend on the action preferences $p(\\cdot)$ according to the expression:\n",
    "\n",
    "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
    "\n",
    "Implement a REINFORCE policy-gradient method for updating the preferences under this policy distribution. The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
    "\n",
    "The agent should be able to use a baseline or not (as defined in the constructor). The `step_size` parameter $\\alpha$ used to update the policy must also be configurable in the constructor.\n",
    "\n",
    "The baseline should track the average reward so far, using the same `step_size` used to update the policy.\n",
    "\n",
    "The `step_size` and whether or not a baseline is used are defined in the constructor by feeding additional arguments in place of `...` below.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class REINFORCE(object):\n",
    "  def __init__(self, name, number_of_arms, step_size=0.1, baseline=False):\n",
    "    self.name = name\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.step_size = step_size\n",
    "    self.use_baseline = baseline\n",
    "    self.reset()\n",
    "\n",
    "  def step(self, previous_action, reward):\n",
    "    # If this is not the first step, update the policy parameters.\n",
    "    if previous_action is not None:\n",
    "        # Compute normalization constant Z = sum_b p(b)^2.\n",
    "        Z = np.sum(self.preferences**2)\n",
    "\n",
    "        # Determine the reward signal (with baseline if enabled).\n",
    "        if self.use_baseline:\n",
    "            # Update baseline using the same step size.\n",
    "            self.baseline += self.step_size * (reward - self.baseline)\n",
    "            delta = reward - self.baseline\n",
    "        else:\n",
    "            delta = reward\n",
    "\n",
    "        # Update preferences for each action.\n",
    "        for b in range(self._number_of_arms):\n",
    "            if b == previous_action:\n",
    "                # Gradient for the taken action.\n",
    "                grad = 2.0 / self.preferences[b] - 2.0 * self.preferences[b] / Z\n",
    "            else:\n",
    "                # Gradient for the actions not taken.\n",
    "                grad = -2.0 * self.preferences[b] / Z\n",
    "            self.preferences[b] += self.step_size * delta * grad\n",
    "\n",
    "    # Compute current policy probabilities using the square-max parameterization.\n",
    "    Z_new = np.sum(self.preferences**2)\n",
    "    probs = (self.preferences**2) / Z_new\n",
    "\n",
    "    # Sample an action from the policy (ties are naturally handled by np.random.choice).\n",
    "    action = np.random.choice(self._number_of_arms, p=probs)\n",
    "    return action\n",
    "\n",
    "  def reset(self):\n",
    "    # Initialize preferences to 1 to ensure valid probabilities (nonzero when squared).\n",
    "    self.preferences = np.ones(self._number_of_arms)\n",
    "    if self.use_baseline:\n",
    "        self.baseline = 0.0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# B) Experiments"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
    "\n",
    "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
    "with a reward on success of 1, and a reward on failure of 0."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment 1: Bernoulli bandit"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture experiment1\n",
    "\n",
    "number_of_arms = 5\n",
    "number_of_steps = 1000\n",
    "\n",
    "agents = [\n",
    "    Random(\n",
    "        \"random\",\n",
    "        number_of_arms),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon=0$\",\n",
    "        number_of_arms,\n",
    "        epsilon=0.),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
    "        number_of_arms,\n",
    "        epsilon=0.1),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/t$\",\n",
    "        number_of_arms,\n",
    "        epsilon=lambda t: 1./t),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
    "        number_of_arms,\n",
    "        epsilon=lambda t: 1./t**0.5),\n",
    "    UCB(\"UCB\",\n",
    "        number_of_arms,\n",
    "        bonus_multiplier=1/np.sqrt(2)),\n",
    "    REINFORCE(\n",
    "        r\"REINFORCE, $\\alpha=0.1$\",\n",
    "        number_of_arms,\n",
    "        step_size=0.1,\n",
    "        baseline=False),\n",
    "    REINFORCE(\n",
    "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
    "        number_of_arms,\n",
    "        step_size=0.1,\n",
    "        baseline=True),\n",
    "]\n",
    "\n",
    "train_agents(agents, number_of_arms, number_of_steps)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiment1.show()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q4 [4 pts total]\n",
    "(Answer inline in the markdown below each question, **within this text cell**.)\n",
    "\n",
    "**[2 pts]**\n",
    "For each algorithm in the plots above, explain whether or not we should be expected it to be good in general, in terms of total regret.\n",
    "\n",
    "## Regret‑Based Evaluation of Algorithms\n",
    "The algorithm's goal is to minimize the regret of the Bernoulli MAB, a 'good' algorithm need to balance exploration and exploitation. That is, it needs to maximize the action that would yeild the most reward and explore all possible actions with a reasonable amount of trials, so that any potential better action is not missed out, we will give a one short summary for each algorithm.\n",
    "\n",
    "- **Greedy (ε=0)**\n",
    "  - Theoretical regret: Θ(T) (linear)\n",
    "  - Expected behavior: No exploration → permanently stuck on whichever arm was sampled first\n",
    "  - Comment: **Bad** — unbounded total regret\n",
    "\n",
    "- **ε‑Greedy (fixed ε)**\n",
    "  - Theoretical regret: Θ(εT) (linear)\n",
    "  - Expected behavior: Constant random exploration → persistent suboptimal pulls\n",
    "  - Comment: **Poor** — regret grows linearly\n",
    "\n",
    "- **ε‑Greedy (ε = 1/t)**\n",
    "  - Theoretical regret: O(log T) (sublinear)\n",
    "  - Expected behavior: Rapidly decreasing exploration rate → sufficient early exploration then almost pure exploitation\n",
    "  - Comment: **Reasonable** — achieves logarithmic regret like UCB but can under‑explore very early if rewards are noisy\n",
    "\n",
    "- **ε‑Greedy (ε≈1/√t)**\n",
    "  - Theoretical regret: O(log T) (sublinear)\n",
    "  - Expected behavior: Gradually shifts from exploration to exploitation → diminishing per‑step regret\n",
    "  - Comment: **Reasonable** — near‑optimal if decay is tuned correctly\n",
    "\n",
    "- **UCB1**\n",
    "  - Theoretical regret: O(log T) (minimax‑optimal)\n",
    "  - Expected behavior: Systematic confidence‑bound exploration → provably minimal cumulative regret\n",
    "  - Comment: **Excellent** — Optimal for regret minimization in this setup\n",
    "\n",
    "- **REINFORCE (Policy Gradient)**\n",
    "  - Theoretical regret: No standard bound\n",
    "  - Expected behavior: High variance updates and slow convergence → large cumulative regret\n",
    "  - Comment::️ **Weak** — not designed to minimize regret\n",
    "\n",
    "- **REINFORCE with Baseline**\n",
    "  - Theoretical regret: No standard bound\n",
    "  - Expected behavior: Reduced variance vs vanilla PG but still lacks sublinear regret scaling\n",
    "  - Comment: **Moderate** — better than plain PG but inferior to UCB and decaying ε‑greedy\n",
    "\n",
    "\n",
    "In this Bernoulli MAB, the empirical results confirm our theoretical regret‑based predictions:\n",
    "\n",
    "- The **Greedy** algorithm exhibits the highest total regret (linear growth), performing worst.\n",
    "- **Fixed ε‑Greedy** likewise incurs large, approximately linear regret.\n",
    "- Both **decaying ε‑Greedy** variants (ε=1/√t and ε=1/t) achieve sublinear regret, reducing cumulative regret over time as expected.\n",
    "- **UCB1** attains the lowest total regret at T=1000, consistent with its optimal O(log T) guarantee.\n",
    "- **REINFORCE with baseline** performs comparably to decaying ε‑Greedy but remains behind UCB1 in regret minimization.\n",
    "\n",
    "\n",
    "**[2 pts]** Explain the relative ranking of the $\\epsilon$-greedy algorithms in this experiment.\n",
    "1. **ε‑Greedy (ε = 1/√t)**\n",
    "   - **Regret bound:** O(log T) (sublinear, asymptotically optimal)\n",
    "   - **Exploration schedule:** Decays slowly enough to ensure sufficient early exploration while rapidly focusing on exploitation\n",
    "   - **Finite‑time behavior:** Lowest cumulative regret among ε‑Greedy variants because it balances exploration/exploitation optimally\n",
    "\n",
    "2. **ε‑Greedy (ε = 0.1)**\n",
    "   - **Regret bound:** Θ(εT) = Θ(T) (linear regret, not asymptotically optimal)\n",
    "   - **Exploration schedule:** Constant exploration → steady but small probability of suboptimal pulls\n",
    "   - **Finite‑time behavior:** Lower regret than rapidly decaying schedules (ε=1/t) for practical horizons, since it avoids premature convergence\n",
    "\n",
    "3. **ε‑Greedy (ε = 1/t)**\n",
    "   - **Regret bound:** O(log T) (sublinear in theory)\n",
    "   - **Exploration schedule:** Decays too quickly → insufficient exploration early on\n",
    "   - **Finite‑time behavior:** High initial regret because it locks in on early estimates; asymptotically optimal but poor practical performance\n",
    "\n",
    "4. **Greedy (ε = 0)**\n",
    "   - **Regret bound:** Θ(T) (linear regret, worst possible)\n",
    "   - **Exploration schedule:** None → permanently exploits first sampled action\n",
    "   - **Finite‑time behavior:** Highest cumulative regret; fails to discover better arms\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
    "\n",
    "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
    "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
    "with a reward on success of 0, and a reward on failure of -1.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture experiment2\n",
    "number_of_arms = 5\n",
    "number_of_steps = 1000\n",
    "\n",
    "train_agents(agents, number_of_arms, number_of_steps,\n",
    "             success_reward=0., fail_reward=-1.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiment2.show()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q5 [2 pts]\n",
    "For each algorithm, note whether the performance changed significantly compared to the **experiment 1**, and explain why it did or did not.\n",
    "\n",
    "(Use at most two sentences per algorithm).\n",
    "\n",
    "- **UCB1 & ε‑Greedy (ε=1/√t):**\n",
    "  Their regret bounds (O(log T)) depend only on relative reward differences, not absolute scale, so performance remains essentially unchanged — they continue to optimally trade off exploration and exploitation.\n",
    "\n",
    "- **Greedy (ε=0):**\n",
    "  Because untried arms start with Q=0 (optimistic relative to their true negative returns), pure greedy implicitly explores until observed rewards drop below 0, yielding far lower regret than in Experiment 1; this is a classic “optimistic initialization” effect.\n",
    "\n",
    "- **ε‑Greedy (ε=0.1):**\n",
    "  Constant exploration still induces linear regret, but finite‑time performance improves marginally because the baseline action-value (0) biases early choices toward unexplored arms longer, reducing early suboptimal pulls compared to Experiment 1.\n",
    "\n",
    "- **ε‑Greedy (ε=1/t):**\n",
    "  Rapidly decaying exploration now benefits from implicit exploration via optimistic initialization, so its finite‑time regret drops significantly; however, it still under‑explores relative to ε=0.1, preserving the same ranking.\n",
    "\n",
    "- **REINFORCE (no baseline):**\n",
    "  With only negative feedback (–1 for failures), vanilla policy gradient suffers high variance updates pushing probabilities downward without clear signal for the optimal arm, slightly degrading learning speed and increasing regret.\n",
    "\n",
    "- **REINFORCE (with baseline):**\n",
    "  The baseline centers returns around zero, allowing positive advantage for arms that avoid failures; this leverages the zero reward on success to accelerate convergence and reduce regret, bringing its performance closer to UCB.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run the following cells"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 3: Non-stationary bandit\n",
    " * Reward on `failure` changes from 0 to +2.\n",
    " * Reward on `success` remains at +1.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture experiment3\n",
    "\n",
    "number_of_arms = 3\n",
    "number_of_steps = 1984\n",
    "agents = [\n",
    "    Random(\n",
    "        \"random\",\n",
    "        number_of_arms),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
    "        number_of_arms,\n",
    "        epsilon=0.1),\n",
    "    EpsilonGreedy(\n",
    "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
    "        number_of_arms,\n",
    "        epsilon=lambda t: 1./t**0.5),\n",
    "    UCB(\"UCB\",\n",
    "        number_of_arms,\n",
    "        bonus_multiplier=1/np.sqrt(2)),\n",
    "    REINFORCE(\n",
    "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
    "        number_of_arms,\n",
    "        step_size=0.1,\n",
    "        baseline=True),\n",
    "\n",
    "]\n",
    "\n",
    "roving_bandit_class = partial(NonStationaryBandit, change_is_good=True)\n",
    "train_agents(agents, number_of_arms, number_of_steps,\n",
    "             bandit_class=roving_bandit_class)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment 4: Non-stationary bandit\n",
    " * Reward on `failure` changes from 0 to +1.\n",
    " * Reward on `success` changes from +1 to 0.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture experiment4\n",
    "\n",
    "number_of_arms = 3\n",
    "number_of_steps = 1984\n",
    "\n",
    "\n",
    "roving_bandit_class = partial(NonStationaryBandit, change_is_good=False)\n",
    "train_agents(agents, number_of_arms, number_of_steps,\n",
    "             bandit_class=roving_bandit_class)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiment3.show()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiment4.show()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q6 [9 pts total]\n",
    "\n",
    "Observe the reward and regret curves above.  After 800 steps, the rewards change. In **experiment 3** `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2.  In **experiment 4**, `success` is now worth 0 and `failure` is worth +1.\n",
    "\n",
    "Below, we ask for explanations.  Answer each question briefly, using at most three sentences per question.\n",
    "\n",
    "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
    "\n",
    "> After 800 steps, “failure” becomes more profitable (+2) than “success” (+1), so algorithms that keep exploring (like ε‑greedy with ε=0.1) quickly discover that the formerly “worse” arms now yield higher reward. In contrast, methods that largely exploited one arm (e.g., UCB with a nearly solid estimate, or ε decaying too fast) fail to update their estimates in time and miss out on the new best arms. Hence, **constant ε‑greedy** stands out as best post‑change, while UCB, greedy, and decaying ε‑greedy lag behind.\n",
    "\n",
    "\n",
    "**[2 pts]** In **experiment 4** explain the ranking in current regret after the change in rewards for all algorithms.\n",
    "\n",
    "> Now “success” drops to 0 and “failure” rises to +1, so the previously optimal arms lose value and must be re‑explored. UCB systematically detects this drop faster than others because its upper confidence terms encourage trials of arms not recently sampled, swiftly adapting to the new reward landscape. REINFORCE and ε‑greedy also eventually adapt, but UCB leads due to its balanced exploration mechanism.\n",
    "\n",
    "\n",
    "**[2 pts]** Explain how and why the current-regret curve for UCB in **experiment 3** differs from the curve in **experiment 4**.\n",
    "\n",
    "> In **experiment 3**, UCB doesn’t see a downward shift in reward for the old “best” arms, so its estimated Q‑values remain high, causing slow adaptation to the new high‑reward “failure” arms. In **experiment 4**, those old arms’ rewards drop to 0, bringing their estimates down and triggering more exploration of other arms. This key difference—estimates decreasing in experiment 4 but not in experiment 3—explains why UCB adapts much more effectively in experiment 4.\n",
    "\n",
    "\n",
    "**[3 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
    "\n",
    "> A common approach is **Discounted UCB** (or “sliding‑window” UCB), which applies higher weight to recent observations and reduces the influence of outdated reward estimates. Concretely, one can either maintain a running window of the most recent samples (resetting arm counts and means periodically) or exponentially discount old rewards in the Q‑value updates. This ensures that when the environment shifts, the algorithm “forgets” stale data and adapts more quickly to new reward patterns.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook3",
    "kind": "private"
   },
   "name": "UCL RL assignment 2025, part I",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2025/RL_assignment_1.ipynb",
     "timestamp": 1741344741505
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2024/RL_assignment_1.ipynb",
     "timestamp": 1740857922938
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2022/RL_assignment_1_solutions.ipynb?workspaceId=hado:ucl2022::citc",
     "timestamp": 1645526852425
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_1_solutions.ipynb?workspaceId=hado:ucl2022::citc",
     "timestamp": 1644958067053
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_1_solutions.ipynb",
     "timestamp": 1644958006320
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_1_solutions.ipynb",
     "timestamp": 1613056961475
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_1.ipynb?workspaceId=mtthss:ucl2020::citc",
     "timestamp": 1579777801613
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_1.ipynb?workspaceId=mtthss:ucl2020::citc",
     "timestamp": 1579714574996
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_1.ipynb?workspaceId=hado:ucl_2020::citc",
     "timestamp": 1579690103674
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_1.ipynb?workspaceId=mtthss:ucl2020::citc",
     "timestamp": 1579689268523
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
