{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part III (25 pts total)\n",
        "---\n",
        "\n",
        "**Name:** Yuan Lu\n",
        "\n",
        "**SN:** 20114649\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *April 10th, 2025*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part3.ipynb`** before the deadline above, where `<studentnumber>` is your student number.\n",
        "\n",
        "----\n",
        "**Reminder of copyrights**\n",
        "\n",
        "Copyrights protect this code/content and distribution or usages of it (or parts of it) without permission is prohibited. This includes uploading it and usage of it in training in any LLMs systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will be guided through the implementation of a full deep reinforcement learning agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVBcO5mAV9Ow"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run all the cells in this section, but do not modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX",
        "ExecuteTime": {
          "end_time": "2025-03-16T22:12:27.874429Z",
          "start_time": "2025-03-16T22:12:27.872826Z"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lpweIqAWBX3"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIRC73HLq6VH"
      },
      "source": [
        "# A) Actor-critics\n",
        "\n",
        "You are going to implement an Actor-critic agent that updates a policy parametrised as a deep neural network.\n",
        "\n",
        "The agent learns online from a single stream of experience, updating the parametes of its policy after each transition in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV03Q3MpveUM"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc-kqp3tveUT",
        "ExecuteTime": {
          "end_time": "2025-03-17T11:13:31.900901Z",
          "start_time": "2025-03-17T11:13:27.240266Z"
        }
      },
      "source": [
        "!pip install -U jaxlib==0.4.33 \"jax[cuda12]\"\n",
        "!git clone https://github.com/deepmind/bsuite.git\n",
        "!pip install bsuite/"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_pTfi5dSFX5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huyKrYpvSHSu",
        "ExecuteTime": {
          "end_time": "2025-03-17T11:17:52.294774Z",
          "start_time": "2025-03-17T11:17:52.263922Z"
        }
      },
      "source": [
        "from bsuite.environments import catch\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6kEki4XHbPy"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYdWwRrbHbcl"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(list_of_episode_returns):\n",
        "  \"\"\"Plot the learning curve.\"\"\"\n",
        "  plt.figure(figsize=(7, 5))\n",
        "\n",
        "  def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "  smoothed_returns = moving_average(list_of_episode_returns, 30)\n",
        "  plt.plot(smoothed_returns)\n",
        "\n",
        "  plt.xlabel('Average episode returns')\n",
        "  plt.xlabel('Number of episodes')\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.spines['left'].set_visible(True)\n",
        "  ax.spines['bottom'].set_visible(True)\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.xaxis.set_ticks_position('bottom')\n",
        "  ax.yaxis.set_ticks_position('left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtlrr5d2p7cS"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "You will use JAX to define a network parametrising:\n",
        "\n",
        "* The values of each state $v(s)$.\n",
        "* The action preferences in each state $\\{p_i(s)\\}_{i\\in\\{1, ..., |A|\\}}$ (you can think of and implement the preferences $\\mathbf{p}(s)$ as a vector output with $|A|$ elements).\n",
        "\n",
        "There are many neural network libraries available that can be used to create networks in jax.  But here we will keep things relatively simple and just define our own network.\n",
        "\n",
        "Specifically, we will define a `network` function returns a **scalar** value `v` and a **vector** of preferences `p`, which will define the policy of the agent.  This function will take as its inputs a dictionary `params` of parameters that we will update, which will contain all the parameters of the network.\n",
        "\n",
        "A lot of the boilerplate code has been written for you.  You will have to implement some functions as indicated in the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj1nsUYuOoe1"
      },
      "source": [
        "### Q1 [3 marks]\n",
        "\n",
        "Create the parameters and define the forward pass of the neural network. The `network` function must look as follows when we call it:\n",
        "`v, p = network(params, observation)`\n",
        "\n",
        "The network should be implemented as follows:\n",
        "* The inputs are a dictionary of parameters `params`, and a tensor (a `jnp.array`) called `observation`.\n",
        "* We reshape the observation into a flat vector `flat_observation`.\n",
        "* We compute a hidden representation\n",
        "`h = relu(flat_observation.dot(w) + b)`, where the weights and biases are given in the input dictionary as `w = params['w']` and `v = params['b']` (and analogously for all other parameters), and where `relu(x)` is a 'rectifier linear unit', which computes $\\max(x, 0)$ elementwise (you can use `jax.nn.relu(x)` to implement this, if you want).\n",
        "* We compute a vector of action preferences as a linear function of `h`, such that `p = h.dot(w_p) + b_p`.\n",
        "* Similarly, we compute a scalar state value `v` as a linear function of `h`.\n",
        "* Finally, we return the scalar value and vector preferences as a tuple.\n",
        "\n",
        "Further, make sure that:\n",
        "* The hidden representation should be a vector of 50 elements.\n",
        "* The action preferences should be a vector of 3 elements (one per each available action).\n",
        "* The value should be a scalar (not a vector with one element).\n",
        "* All parameters should be initialised randomly as follows:\n",
        "```\n",
        "rng_key, param_key = jax.random.split(rng_key)\n",
        "parameter = jax.random.truncated_normal(param_key, -1, 1, shape)\n",
        "```\n",
        "where `shape` is the relevant shape for this `parameter` (e.g., `parameter` is `w` or `b`, etc.).  The first line is important: it 'splits' the random key into a temporary random key `param_key` that should **only** be used for this parameter, and a new `rng_key` that can be split again later for the next parameters, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF-6IWP30nip"
      },
      "outputs": [],
      "source": [
        "def create_parameters(rng_key, observation):\n",
        "  \"\"\"Creates and returns a dictionary of network parameters.\n",
        "\n",
        "  Parameters:\n",
        "    rng_key: a JAX random key.\n",
        "    observation: a JAX array representing a sample observation, used to determine the input size.\n",
        "\n",
        "  Returns:\n",
        "    params: a dictionary containing the following keys:\n",
        "        'w'   : weights for the hidden layer (shape: [input_dim, 50])\n",
        "        'b'   : biases for the hidden layer (shape: [50])\n",
        "        'w_p' : weights for the action preference layer (shape: [50, 3])\n",
        "        'b_p' : biases for the action preference layer (shape: [3])\n",
        "        'w_v' : weights for the state value layer (shape: [50, 1])\n",
        "        'b_v' : biases for the state value layer (shape: [1])\n",
        "  \"\"\"\n",
        "  # We first flatten the observation so we know its size.\n",
        "  in_dim = jnp.size(observation)\n",
        "  hidden_dim = 50\n",
        "  num_actions = 3\n",
        "\n",
        "  params = {}\n",
        "  # 1) Hidden-layer weights and bias:\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['w'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(in_dim, hidden_dim))\n",
        "\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['b'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(hidden_dim,))\n",
        "\n",
        "  # 2) Action-preferences head weights/bias:\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['w_p'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(hidden_dim, num_actions))\n",
        "\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['b_p'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(num_actions,))\n",
        "\n",
        "  # 3) Value head weights/bias:\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['w_v'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(hidden_dim, 1))\n",
        "\n",
        "  rng_key, param_key = jax.random.split(rng_key)\n",
        "  params['b_v'] = jax.random.truncated_normal(\n",
        "      param_key, lower=-1, upper=1, shape=(1,))\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRBP0xjtQvTi"
      },
      "outputs": [],
      "source": [
        "def network(params, observation):\n",
        "  \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "  Parameters:\n",
        "    params: a dictionary of network parameters.\n",
        "    observation: a JAX array representing the current state.\n",
        "\n",
        "  Returns:\n",
        "    v: a scalar value estimate of the state.\n",
        "    p: a vector of action preferences.\n",
        "  \"\"\"\n",
        "  # Flatten the observation\n",
        "  flat_observation = jnp.reshape(observation, [-1])\n",
        "\n",
        "  # Hidden representation\n",
        "  h = jax.nn.relu(flat_observation @ params['w'] + params['b'])\n",
        "\n",
        "  # Action preferences\n",
        "  p = h @ params['w_p'] + params['b_p']\n",
        "\n",
        "  # Scalar value\n",
        "  v = h @ params['w_v'] + params['b_v']    # shape: (1,)\n",
        "  v = jnp.squeeze(v)  # Make it a scalar instead of [scalar]\n",
        "  return v, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImJUgzFosvD"
      },
      "source": [
        "### Choosing actions\n",
        "\n",
        "A critical component of an actor-critic agent is a (stochastic) policy, mapping `observations` to `actions`.\n",
        "\n",
        "In deep RL, this mapping is conventionally parametrised by a deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVXNWwlYW24"
      },
      "source": [
        "Here we provide a softmax policy parametrised by the neural network above (i.e., using the `network` function). The function has signature `action = softmax_policy(parameters, key, obs)`, taking the current network parameters `parameters`, a JAX random `key` and the current `observation`.\n",
        "\n",
        "### Softmax definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSOO10b20yD5"
      },
      "outputs": [],
      "source": [
        "#DO NOT MODIFY\n",
        "# Functions to perform random sampling in JAX (e.g. those in `jax.random`) take\n",
        "# a random key as input, and they are deterministic function of such a key. In\n",
        "# general, in a JAX program you need to use the `jax.random.split` function to\n",
        "# generate new random keys before every new sampling. The run loop that runs the\n",
        "# experiment later on will split and provides the keys for the function defined\n",
        "# here.\n",
        "\n",
        "# Note that we 'jit' the function.  This means the function will be compiled,\n",
        "# which will make it run faster. This does also suppress print statements, so if\n",
        "# you are debugging and want to print please comment out the `@jax.jit` line,\n",
        "# but don't forget to put it back before running the experiment below, and\n",
        "# especially before submitting your assignment.\n",
        "\n",
        "@jax.jit\n",
        "def softmax_policy(parameters, key, obs):\n",
        "  \"\"\"Sample action from a softmax policy.\"\"\"\n",
        "  _, p = network(parameters, obs)\n",
        "  return jax.random.categorical(key, p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYcb7Y9krnT"
      },
      "source": [
        "### Learning values and policies\n",
        "\n",
        "An actor-critic agent requires to update the parameters of the network so as to simultaneously improve the value predictions and the policy.\n",
        "\n",
        "In the next section you will define the gradient updates for each of these two components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIlpmNEk5fv"
      },
      "source": [
        "### Q3 [4 marks]\n",
        "\n",
        "Implement a function to compute a stochastic estimate of the policy gradient from a 1 step transition in the environment.\n",
        "\n",
        "* You will use $R_{t+1} + \\gamma v(S_{t+1})$ as an estimate of $q_{\\pi}(S_t, A_t)$\n",
        "* You will use $v(S_{t})$ as a baseline to reduce the variance of the updates.\n",
        "\n",
        "In the code we actually use names `obs_tm1`, `a_tm1` (where `tm1` stands for '$t$ minus one') for the observation and action at time $t-1$, and `r_t`, `discount_t`, `obs_t` for the reward, discount, and observation at time $t$. So the code is offset a single time step in terms of naming as compared to the typical mathematical formulations.  This is just a naming convention, and should not impact the algorithm.\n",
        "\n",
        "The function below must therefore have signature `grads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * `parameters`: the parameters of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and discount `discount_t` and observation `obs_t`, as obsesrved after taking action `a_tm1`.\n",
        "\n",
        "This function should return a stochastic estimate of the policy gradient, where `grads` has the same structure as `parameters` and contains an estimate of the gradient of the expected episodic return wrt to each parameter.\n",
        "\n",
        "The policy-gradient estimate should use bootstrapping, using the value estimates that can be gotten using the saame `parameters` as used for the policy.  So the output of this function can be used in a one-step actor-critic update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02RJAKiX85Wx"
      },
      "source": [
        "### Jax hint:\n",
        "Note that you can use `jax.grad(f)` to get the gradient of any (pure) jax function with a scalar output.  For instance, consider:\n",
        "\n",
        "        def f(w, x, y):\n",
        "          # w, x, and y are all vectors\n",
        "          return jnp.sum(w*x + y)\n",
        "\n",
        "        df = jax.grad(f)\n",
        "    \n",
        "then calling `df(w, x, y)` will give the gradient of the output of `f(w, x, y)` with respect to the first input argument --- here called `w`. You can use this new function `df` as just a normal function. For instance, it can be called from other functions, as usual.\n",
        "\n",
        "Run the cell below to see a concrete example in action.  Note that `df(w, x, y)` in the example below evaluates to the same values as `x`.  This is correct, because the gradient of `f` with respect to `w` is indeed `x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1TjveqqGnNA"
      },
      "outputs": [],
      "source": [
        "def f(w, x, y):\n",
        "  return jnp.sum(w*x + y)\n",
        "\n",
        "df = jax.grad(f)\n",
        "\n",
        "w = jnp.array([1., 2.])\n",
        "x = jnp.array([3., 5.])\n",
        "y = jnp.array([7., 11.])\n",
        "\n",
        "print(f'f(w, x, y):  {f(w, x, y)}')\n",
        "print(f'df(w, x, y): {df(w, x, y)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wngNEiwrQeF0"
      },
      "source": [
        "### Jax hint:\n",
        "You network will have a dictionary `params` as input.  Suppose you have a function that computes the output of a network, and then uses this to do some stuff, and you want the gradient of that new function.  That is, suppose your code looks something like this:\n",
        "\n",
        "        # Define network\n",
        "        def network(params, ...):\n",
        "          ...\n",
        "\n",
        "        # Define a new function\n",
        "        def function(params, x, y, z, ...):\n",
        "           ...\n",
        "           output = network(params, x)\n",
        "           ...\n",
        "           return ...(some function of output)...        \n",
        "\n",
        "Now,\n",
        "```\n",
        "grads = jax.grad(function)(params, x, y, z, ...)\n",
        "```\n",
        "will give the gradients of `function` with respect to the first input argument, `params`.  These gradients `grads` will have exactly the same shape as the input argment `params`.\n",
        "\n",
        "But note that we cannot do things like `params + grads`, because `params` and `grads` are both dictionaries, not just `jnp.array`s.  Instead, we could explicitly traverse the dictionary.  But it is perhaps even easier to use in-built tree utils in Jax.  Specifically, the util `jax.tree_util.tree_map(f, x, y)` applies function f to all the elements in `x` and `y`.\n",
        "\n",
        "For instance, to add the gradients to the parameters, we can use:\n",
        "```\n",
        "def add_gradient_to_weight(w, g, learning_rate=0.1):\n",
        "  return w + learning_rate * g\n",
        "new_params = jax.tree_map(add_gradient_to_weight, params, grads)\n",
        "```\n",
        "The above snippet would apply the function `add_gradient_to_weight` to the elements in `params` and `grads`, here adding each element in `grads` to each corresponding element in `weights` (in this case using the default learning rate `learning_rate=0.1`, as specified in the function definition).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    \"\"\"\n",
        "    Computes a stochastic estimate of the policy gradient using a one-step actor–critic update.\n",
        "\n",
        "    Inputs:\n",
        "      parameters   : network parameters\n",
        "      obs_tm1      : observation at time t-1 (when action a_tm1 was selected)\n",
        "      a_tm1        : action taken at time t-1\n",
        "      r_t          : reward received after taking a_tm1\n",
        "      discount_t   : discount factor at time t\n",
        "      obs_t        : observation at time t (after taking action a_tm1)\n",
        "\n",
        "    The pseudo-loss is defined as:\n",
        "      loss = - log π(a_tm1|obs_tm1) * (r_t + discount_t * v(obs_t) - v(obs_tm1))\n",
        "\n",
        "    Its gradient (when multiplied by -1) yields:\n",
        "      ∇ log π(a_tm1|obs_tm1) * (r_t + discount_t*v(obs_t) - v(obs_tm1))\n",
        "\n",
        "    which is our desired policy gradient estimate.\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        # Compute value and preferences at the previous observation.\n",
        "        v_tm1, p_tm1 = network(params, obs_tm1)\n",
        "        # Compute value for the next observation; stop gradients here\n",
        "        v_t, _ = network(params, obs_t)\n",
        "        # Define target as r_t + γ * v(obs_t), with no gradient flowing through v(obs_t)\n",
        "        target = r_t + discount_t * jax.lax.stop_gradient(v_t)\n",
        "        # Compute the advantage using v(obs_tm1) as baseline (also stopping its gradient)\n",
        "        advantage = target - jax.lax.stop_gradient(v_tm1)\n",
        "        # Compute log probabilities from the preference logits\n",
        "        log_probs = jax.nn.log_softmax(p_tm1)\n",
        "        log_prob = log_probs[a_tm1]\n",
        "        # The pseudo-loss is -log_prob * advantage\n",
        "        return -log_prob * advantage\n",
        "\n",
        "    # Compute the gradient of the loss with respect to parameters\n",
        "    grads = jax.grad(loss_fn)(parameters)\n",
        "    # Return the negative of that gradient so that a gradient descent step performs policy ascent.\n",
        "    return jax.tree_util.tree_map(lambda g: -g, grads)\n"
      ],
      "metadata": {
        "id": "YimDgbwbOkrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XgjefrBlIGN"
      },
      "source": [
        "### Q4 [4 marks]\n",
        "\n",
        "Implement a function to compute a TD(0) update for the parameters of the value function.\n",
        "\n",
        "It must have signature `td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * the current parameters `parameters` of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and environment discount `discount_t`\n",
        "  * and the following observation `obs_t`\n",
        "* Returns a stochastic TD(0) semi-gradient update: `td_update` has the same structure as `parameters`. This contains a stochastic estimate of the negative semi-gradient of the expected value prediction loss: a TD(0) update."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    \"\"\"\n",
        "    Returns the negative semi-gradient of the 1-step TD(0) loss:\n",
        "      L(θ) = 0.5 * (r_t + γ * v(obs_t)_stop_grad - v(obs_tm1))^2,\n",
        "    with no gradient flowing through v(obs_t).\n",
        "    \"\"\"\n",
        "    def loss_fn(params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "        # Compute the value for the next observation, then freeze its gradient.\n",
        "        v_t, _ = network(params, obs_t)\n",
        "        target = r_t + discount_t * jax.lax.stop_gradient(v_t)\n",
        "        # Compute the current state's value.\n",
        "        v_tm1, _ = network(params, obs_tm1)\n",
        "        # Squared TD error.\n",
        "        return 0.5 * (target - v_tm1) * v_tm1#** 2\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the parameters.\n",
        "    grads = jax.grad(loss_fn)(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "\n",
        "    # Return the negative gradient so that a gradient descent update reduces the TD error.\n",
        "    return jax.tree_util.tree_map(lambda g: g, grads)\n"
      ],
      "metadata": {
        "id": "aJd0tNHHML9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXgLZg8mfMd"
      },
      "source": [
        "### Updating shared parameters\n",
        "\n",
        "The policy gradient identifies the direction of change in the parameters that most steeply improve the policy.\n",
        "The value update identifies the direction of change in the parameters that improves the value predictions (according to TD).\n",
        "\n",
        "However, the value and policy share some of the parameters of the network.  How do we combine the two gradient updates?\n",
        "\n",
        "In this assignment, we simply sum the policy and value components.\n",
        "The function that combines the two gradients is implemented for you in the cell below.  Note the use of `jax.tree_map` to facilitate adding the structured parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhKCLe8jjkdZ"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_util.tree_map(lambda pg, td: pg + td, pgrads, td_update)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYaVb6GcpCRe"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "In deep learning, gradient updates are typically rescaled and modifed to avoid taking too large a step on a single update (e.g., due to large variance), and to facilitate the optimisation process (it turns out raw stochatic gradients are often not the most effective for updating neural networks).\n",
        "\n",
        "For instance given a candidate gradient update $\\nabla$ we may update our parameters $\\theta$ by;\n",
        "$$\\Delta \\theta = \\theta + \\alpha * \\nabla\\,,$$\n",
        "where $\\alpha$ is a small number between 0 and 1 (e.g., $\\alpha=0.01$ or $\\alpha=0.001$), referred to as `step_size` or `learning_rate`\n",
        "\n",
        "The gradients with respect to each weight of a neural network may however have very different magnitudes. This can make it hard to set a suitable learning rate $\\alpha$.\n",
        "\n",
        "In deep learning, and deep RL, we typically use adaptive learning rates, for instance by rescaling each component of the gradient using statistics tracking the typical size of the updates to that weight. Then the entire update is rescaled using a global `learning_rate` $\\alpha$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFePWfApaMU"
      },
      "source": [
        "### Q5 [2 marks]\n",
        "\n",
        "A popular approach to adaptive gradient rescaling was introduced by the `Adam` algorithm.\n",
        "This algorithm implements the following procedure before applying each update:\n",
        "* Increase an update counter $k \\gets k+1$ (starting at k=0 before any updates),\n",
        "* Update the first moment of each gradient component $\\mu \\gets (1 - \\beta_1) g + \\beta_1 \\mu$ where $g$ is the latest stochastic gradient, where $\\beta_1$ is a parameter for the moving average.\n",
        "* Update the second moment of each gradient component $\\nu_i = (1 - \\beta_2) g_i ^ 2 + \\beta_2 \\nu_i $ where $g$ is the latest gradient update, where $\\beta_2$ is a parameter for the moving average.\n",
        "* Use the following update to update the weights:\n",
        "$$\\Delta w = \\alpha \\frac{\\mu / (1 - \\beta_1 ^ {k})}{\\epsilon + \\sqrt{\\nu / (1 - \\beta_1 ^ {k})}}$$\n",
        "* $\\alpha$ is a global `learning rate`\n",
        "* $\\beta_1$ and $\\beta_2$ define a soft horizon for the per-weight statistics.\n",
        "* $\\epsilon$ makes the rescaling more robust to numerical issues.\n",
        "\n",
        "(See [Kingma et al, 2014](https://arxiv.org/abs/1412.6980) for details, if you are interested.)\n",
        "\n",
        "In the next cell define a pair of functions (`opt_init`, and `opt_update` --- in each case `opt` is short for 'optimiser'), where:\n",
        "\n",
        "The `opt_init` function has signature `opt_state = opt_init(parameters)`.\n",
        "* Takes the network parameters as inputs\n",
        "* Initialises an `optimiser state` holding the per weight statistics.\n",
        "\n",
        "The `opt_update` function has signature `updates, opt_state = opt_update(grads, opt_state)`.\n",
        "* Takes a `gradient` and an `optimisers state`,\n",
        "* and returns the transformed gradient and the updated `optimiser state`.\n",
        "\n",
        "The optimiser state `opt_state` should contain:\n",
        "* The first-order momentum $\\mu$, as updated with a moving-average-parameter $\\beta_1$ which we call `b1` in the code.\n",
        "* The first-order momentum $\\nu$, as updated with a moving-average-parameter $\\beta_2$ which we call `b2` in the code.\n",
        "\n",
        "We will ignore the initial correction Adam typically uses, and will instead use the simpler transformation:\n",
        "$$\\Delta w = \\alpha \\frac{\\mu}{\\epsilon + \\sqrt{\\nu}}$$\n",
        "\n",
        "Set the algorithm's hyper-parameters to $\\alpha=0.003$, $\\beta_1=.9$ and $\\beta_2=.999$, $\\epsilon=10^{-4}$.  You are allowed to hard-code these in, or make them configurable (e.g., pass them as additional arguments to `opt_update`, which is better if you want to play around, of course).  Set the initial moving averages to zero for $\\mu$ and one for $\\nu$.  E.g.,\n",
        "\n",
        "        mu = jax.tree_map(jnp.zeros_like, parameters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def opt_init(parameters):\n",
        "  \"\"\"\n",
        "  Initialize the optimizer state (mu, nu) for Adam.\n",
        "  mu is the first-moment estimate, nu is the second-moment estimate.\n",
        "  We'll start mu at 0, nu at 1 as suggested.\n",
        "  \"\"\"\n",
        "  mu = jax.tree_util.tree_map(jnp.zeros_like, parameters)\n",
        "  nu = jax.tree_util.tree_map(jnp.ones_like, parameters)\n",
        "  k = jnp.array(0)\n",
        "  return (mu, nu, k)\n",
        "\n",
        "@jax.jit\n",
        "def opt_update(grads, opt_state):\n",
        "  \"\"\"\n",
        "  Given the raw gradients and the optimizer state, return the (transformed) updates\n",
        "  and the new optimizer state.  We implement a simplified Adam:\n",
        "      mu     = b1 * mu + (1-b1) * g\n",
        "      nu     = b2 * nu + (1-b2) * (g^2)\n",
        "      update = alpha * [mu / sqrt(nu + eps)]\n",
        "  \"\"\"\n",
        "\n",
        "  mu_prev, nu_prev, k_prev = opt_state\n",
        "  beta1, beta2 = 0.9, 0.999\n",
        "  alpha, eps = 0.003, 1e-4\n",
        "\n",
        "  mu_new = jax.tree_util.tree_map(lambda m, g: beta1 * m + (1 - beta1) * g, mu_prev, grads)\n",
        "  nu_new = jax.tree_util.tree_map(lambda v, g: beta2 * v + (1 - beta2) * (g ** 2), nu_prev, grads)\n",
        "  k_new = k_prev + 1\n",
        "\n",
        "  updates = jax.tree_util.tree_map(lambda m, v: alpha * m / (eps + jnp.sqrt(v)), mu_new, nu_new)\n",
        "  return updates, (mu_new, nu_new, k_new)"
      ],
      "metadata": {
        "id": "Z_nSZwaVVQ--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DczWvZfNSnTj"
      },
      "source": [
        "### Run experiments\n",
        "\n",
        "Run the cell below to show the performance of the resulting agent.\n",
        "\n",
        "You may also use this section for debugging your implementations.\n",
        "\n",
        "Note however, that most functions are `jitted` for performance,\n",
        "* either using the `@jax.jit` decorator in the function definition\n",
        "* or calling explicitly `fn = jax.jit(fn)`\n",
        "\n",
        "When jitting, the code is compiled on the first time the function is executed\n",
        "* and execution is much faster on subsequent calls.\n",
        "* a notable side effect is that print statements in a jitted function will only execute on the first execution of the function.\n",
        "* to drop into a debugger or print on each function execution you will have to disable the `@jax.jit` annotations and jax.jit calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz837XTkLxE8"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "# Experiment configs.\n",
        "train_episodes = 5000\n",
        "discount_factor = .99\n",
        "\n",
        "# Create environment.\n",
        "env = catch.Catch(seed=42)\n",
        "\n",
        "# Build and initialize network.\n",
        "rng = jax.random.PRNGKey(44)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "sample_input = env.observation_spec().generate_value()\n",
        "parameters = create_parameters(init_rng, sample_input)\n",
        "\n",
        "# Initialize optimizer state.\n",
        "opt_state = opt_init(parameters)\n",
        "\n",
        "# Apply updates\n",
        "def apply_updates(params, updates):\n",
        "  return jax.tree_util.tree_map(lambda p, u: p + u, params, updates)\n",
        "\n",
        "# Jit.\n",
        "opt_update = jax.jit(opt_update)\n",
        "apply_updates = jax.jit(apply_updates)\n",
        "\n",
        "print(f\"Training agent for {train_episodes} episodes...\")\n",
        "all_episode_returns = []\n",
        "\n",
        "for _ in range(train_episodes):\n",
        "  episode_return = 0.\n",
        "  timestep = env.reset()\n",
        "  obs_tm1 = timestep.observation\n",
        "\n",
        "  # Sample initial action.\n",
        "  rng, policy_rng = jax.random.split(rng)\n",
        "  a_tm1 = softmax_policy(parameters, policy_rng, obs_tm1)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Step environment.\n",
        "    new_timestep = env.step(int(a_tm1))\n",
        "\n",
        "    # Sample action from agent policy.\n",
        "    rng, policy_rng = jax.random.split(rng)\n",
        "    a_t = softmax_policy(parameters, policy_rng, new_timestep.observation)\n",
        "\n",
        "    # Update params.\n",
        "    r_t = new_timestep.reward\n",
        "    discount_t = discount_factor * new_timestep.discount\n",
        "    dJ_dtheta = compute_gradient(\n",
        "        parameters, obs_tm1, a_tm1, r_t, discount_t,\n",
        "        new_timestep.observation)\n",
        "    updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
        "    parameters = apply_updates(parameters, updates)\n",
        "\n",
        "    # Within episode book-keeping.\n",
        "    episode_return += new_timestep.reward\n",
        "    timestep = new_timestep\n",
        "    obs_tm1 = new_timestep.observation\n",
        "    a_tm1 = a_t\n",
        "\n",
        "  # Experiment results tracking.\n",
        "  all_episode_returns.append(episode_return)\n",
        "\n",
        "# Plot learning curve.\n",
        "plot_learning_curve(all_episode_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dowFJ_l-32A"
      },
      "source": [
        "# B) An alternative update\n",
        "\n",
        "You are going to implement a different kind of agent.\n",
        "\n",
        "Like an actor-critic, it learns online from a single stream of experience, updating the parametes after each transition in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQD0Qw8-_QJy"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "\n",
        "The agent will reuse the same neural network we defined for the actor-critic:\n",
        "* the scalar output will be trained via TD to estimate state values\n",
        "* the vector preferences `p` will be updated according to a different rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6j0AIF8GhdR"
      },
      "source": [
        "### Choosing actions\n",
        "\n",
        "As in actor-critics, the (stochastic) mapping from `observations` to `actions` depends on the vector of preferences `p` from the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE00LhDL_nbC"
      },
      "source": [
        "### Q6 [2 marks]\n",
        "\n",
        "The new agent's policy will have the signature `action = epsilon_greedy_policy(parameters, key, observation)`,\n",
        "* Take as inputs the current network parameters `parameters`, a JAX random `key` and the current `observation`\n",
        "* Return with probability `0.9` the greedy `action` with respect to the preferences `p`, ties must be broken at random.\n",
        "* Return an action uniformly at random with probability `0.1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwTDROL_Lx5"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def epsilon_greedy_policy(parameters, key, observation):\n",
        "    \"\"\"Sample action from an epsilon-greedy policy.\"\"\"\n",
        "    _, p = network(parameters, observation)\n",
        "    max_p = jnp.max(p)\n",
        "    mask = p == max_p\n",
        "    # Create logits where max entries are 0, others -infinity\n",
        "    greedy_logits = jnp.where(mask, 0.0, -jnp.inf)\n",
        "    # Split keys for decision and actions\n",
        "    key, subkey = jax.random.split(key)\n",
        "    do_greedy = jax.random.bernoulli(subkey, 0.9)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    greedy_key, random_key = jax.random.split(subkey)\n",
        "    # Greedy action: sample from max indices\n",
        "    greedy_action = jax.random.categorical(greedy_key, greedy_logits)\n",
        "    # Random action: uniform over all actions\n",
        "    random_action = jax.random.randint(random_key, (), 0, p.shape[-1])\n",
        "    # Choose action\n",
        "    action = jnp.where(do_greedy, greedy_action, random_action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1zQiRX5BfeA"
      },
      "source": [
        "### Q7 [3 marks]\n",
        "\n",
        "The parameters $w_p$ of the preferences $p_{w_p}(s, a)$ will be update according to the following gradient-based update:\n",
        "\n",
        "$$\\Delta w_p = \\alpha (R_{t+1} + \\gamma v(S_{t+1}) - p(S_{t}, A_t)) \\nabla p(S_t, A_t))$$\n",
        "\n",
        "where `v` is the state value trained by TD as in the actor critic.\n",
        "\n",
        "You musy implement this in the function `preference_gradient`, with the same signature as `policy_gradient`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def preference_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    # Compute target = R + γ * v(S'), with v(S') from network but stop gradient\n",
        "    v_t, _ = network(parameters, obs_t)\n",
        "    target = r_t + discount_t * jax.lax.stop_gradient(v_t)\n",
        "\n",
        "    # Get p(S_tm1, a_tm1)\n",
        "    _, p_tm1 = network(parameters, obs_tm1)\n",
        "    p_a = p_tm1[a_tm1]  # Fixed: use p_tm1 instead of p\n",
        "\n",
        "    # Compute gradient of p(S_tm1, a_tm1) with respect to parameters\n",
        "    def loss_fn(params):\n",
        "        _, p_tm1_internal = network(params, obs_tm1)\n",
        "        return p_tm1_internal[a_tm1]\n",
        "\n",
        "    grad_p = jax.grad(loss_fn)(parameters)\n",
        "\n",
        "    # Scale gradient by (target - p_a)\n",
        "    delta = target - p_a\n",
        "    scaled_grad = jax.tree_util.tree_map(lambda g: delta * g, grad_p)\n",
        "\n",
        "    return scaled_grad\n"
      ],
      "metadata": {
        "id": "khM6uR22x25s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4RFKnVF8E8"
      },
      "source": [
        "### Updating shared parameters\n",
        "\n",
        "Just like in the actor critic the overall update to the parameters is a combination of two quantities:\n",
        "* the new update we defined for the vector of preferences\n",
        "* the same TD update to the scalar output that we used in the actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnghCzKhF-gI"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = preference_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  vgrads = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_util.tree_map(lambda pg, td: pg + td, pgrads, vgrads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44QBKEF4HFK8"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "The gradient updates are rescaled using the same optimiser used for the actor-critic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbdHQo23FJit"
      },
      "source": [
        "### Run experiments\n",
        "\n",
        "Run the cell below to show the performance of the new agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ept5NG1oFLnu"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "# Experiment configs.\n",
        "train_episodes = 5000\n",
        "discount_factor = .99\n",
        "\n",
        "# Create environment.\n",
        "env = catch.Catch(seed=42)\n",
        "\n",
        "# Build and initialize network.\n",
        "rng = jax.random.PRNGKey(44)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "sample_input = env.observation_spec().generate_value()\n",
        "parameters = create_parameters(init_rng, sample_input)\n",
        "\n",
        "# Initialize optimizer state.\n",
        "opt_state = opt_init(parameters)\n",
        "\n",
        "# Apply updates\n",
        "def apply_updates(params, updates):\n",
        "  return jax.tree_util.tree_map(lambda p, u: p + u, params, updates)\n",
        "\n",
        "# Jit.\n",
        "opt_update = jax.jit(opt_update)\n",
        "apply_updates = jax.jit(apply_updates)\n",
        "\n",
        "print(f\"Training agent for {train_episodes} episodes...\")\n",
        "all_episode_returns = []\n",
        "\n",
        "for _ in range(train_episodes):\n",
        "  episode_return = 0.\n",
        "  timestep = env.reset()\n",
        "  obs_tm1 = timestep.observation\n",
        "\n",
        "  # Sample initial action.\n",
        "  rng, policy_rng = jax.random.split(rng)\n",
        "  a_tm1 = epsilon_greedy_policy(parameters, policy_rng, obs_tm1)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Step environment.\n",
        "    new_timestep = env.step(int(a_tm1))\n",
        "\n",
        "    # Sample action from agent policy.\n",
        "    rng, policy_rng = jax.random.split(rng)\n",
        "    a_t = epsilon_greedy_policy(parameters, policy_rng, new_timestep.observation)\n",
        "\n",
        "    # Update params.\n",
        "    r_t = new_timestep.reward\n",
        "    discount_t = discount_factor * new_timestep.discount\n",
        "\n",
        "    dJ_dtheta = compute_gradient(\n",
        "        parameters, obs_tm1, a_tm1, r_t, discount_t,\n",
        "        new_timestep.observation)\n",
        "    updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
        "    parameters = apply_updates(parameters, updates)\n",
        "\n",
        "    # Within episode book-keeping.\n",
        "    episode_return += new_timestep.reward\n",
        "    timestep = new_timestep\n",
        "    obs_tm1 = new_timestep.observation\n",
        "    a_tm1 = a_t\n",
        "\n",
        "  # Experiment results tracking.\n",
        "  all_episode_returns.append(episode_return)\n",
        "\n",
        "# Plot learning curve.\n",
        "plot_learning_curve(all_episode_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8 [8 marks]\n",
        "\n",
        "[2pts]: Run the optimisation in Part A) with both normal SGD (with a constant learning rate = 1e-3) and the ADAM optimisation you implemented. What can you observe? Explain any difference you might see. Please include both plots your answer (one for SGD and the other one for ADAM).\n",
        "\n",
        "> When running our experiments, we observe that the ADAM optimizer results in a steadily improving agent performance, with the average episode return growing from around –1 to roughly 1. In contrast, when training with standard SGD (using a fixed learning rate of 1e-3), the performance remains erratic—returns tend to oscillate between –1 and –0.2 without a clear upward trend (see the figire below).\n",
        "\n",
        "Here is a potential explainantion of what is happening:\n",
        "- ADAM automatically adapts the learning rate for each parameter by keeping track of both the first moment (the mean of gradients) and the second moment (the uncentered variance). This means that each parameter update is scaled according to how noisy or consistent the gradient signal is.\n",
        "\n",
        "- SGD applies a constant learning rate across all parameters. Without any mechanism to adapt the step size or account for the variance in the gradients, SGD can overshoot in some directions and take steps that are too small in others.\n",
        "\n",
        "- In actor–critic architectures, the network jointly learns both the policy and the value function. Because these two components share parameters, any instability in the gradient updates can have a compounded effect on learning. ADAM’s ability to perform per-parameter learning rate adjustment allows it to balance these updates more effectively than SGD, so it has better convergence properties.\n",
        "\n",
        "- The semi-gradient methods we use in TD-learning (for the critic) and the policy gradient estimates (for the actor) in it self sensitive to the scale of the updates. If the learning rate is not well-tuned—as is the case with a constant SGD learning rate—the combined update can fail to reduce the TD error or improve the policy, resulting in oscillatory or stagnant learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "pHcXynREUYgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %matplotlib inline\n",
        "# %%capture\n",
        "# @title Code for SGD\n",
        "def sgd_opt_init(parameters):\n",
        "  \"\"\"\n",
        "  Initialize the optimizer state for SGD.\n",
        "  For SGD with a constant learning rate, no auxiliary state is needed,\n",
        "  but we'll include an iteration counter for consistency.\n",
        "  \"\"\"\n",
        "  k = jnp.array(0)\n",
        "  return k\n",
        "\n",
        "@jax.jit\n",
        "def sgd_opt_update(grads, opt_state):\n",
        "  \"\"\"\n",
        "  Given the gradients and the optimizer state, return the updates\n",
        "  and the new optimizer state.\n",
        "\n",
        "  We implement SGD with a constant learning rate:\n",
        "      update = -lr * grad\n",
        "  where lr = 1e-3.\n",
        "  \"\"\"\n",
        "  lr = 1e-3\n",
        "  k = opt_state\n",
        "  k_new = k + 1\n",
        "  updates = jax.tree_util.tree_map(lambda g: -lr * g, grads)\n",
        "  return updates, k_new\n",
        "\n",
        "@jax.jit\n",
        "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_util.tree_map(lambda pg, td: pg + td, pgrads, td_update)\n",
        "\n",
        "def run_training(opt_update_fn, opt_init_fn, train_episodes, discount_factor):\n",
        "    \"\"\"\n",
        "    A training loop that runs a fixed number of episodes using the provided optimizer.\n",
        "    \"\"\"\n",
        "    print(f\"Training with {opt_init_fn.__name__} for {train_episodes} episodes...\")\n",
        "    env = catch.Catch(seed=42)\n",
        "    rng = jax.random.PRNGKey(44)\n",
        "    rng, init_rng = jax.random.split(rng)\n",
        "    sample_input = env.observation_spec().generate_value()\n",
        "    parameters = create_parameters(init_rng, sample_input)\n",
        "    opt_state = opt_init_fn(parameters)\n",
        "    episode_returns = []\n",
        "\n",
        "    for _ in range(train_episodes):\n",
        "        episode_return = 0.0\n",
        "        timestep = env.reset()\n",
        "        obs_tm1 = timestep.observation\n",
        "        rng, policy_rng = jax.random.split(rng)\n",
        "        a_tm1 = softmax_policy(parameters, policy_rng, obs_tm1)\n",
        "        while not timestep.last():\n",
        "            new_timestep = env.step(int(a_tm1))\n",
        "            rng, policy_rng = jax.random.split(rng)\n",
        "            a_t = softmax_policy(parameters, policy_rng, new_timestep.observation)\n",
        "            r_t = new_timestep.reward\n",
        "            discount_t = discount_factor * new_timestep.discount\n",
        "            grads = compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, new_timestep.observation)\n",
        "            updates, opt_state = opt_update_fn(grads, opt_state)\n",
        "            parameters = apply_updates(parameters, updates)\n",
        "            episode_return += new_timestep.reward\n",
        "            timestep = new_timestep\n",
        "            obs_tm1 = new_timestep.observation\n",
        "            a_tm1 = a_t\n",
        "        episode_returns.append(episode_return)\n",
        "    return episode_returns\n",
        "\n",
        "train_episodes = 5000\n",
        "discount_factor = 0.99\n",
        "\n",
        "# Run training with ADAM\n",
        "adam_returns = run_training(opt_update, opt_init, train_episodes, discount_factor)\n",
        "# Run training with SGD\n",
        "sgd_returns = run_training(sgd_opt_update, sgd_opt_init, train_episodes, discount_factor)\n",
        "\n",
        "def plot_learning_curve_custom(adam_returns, sgd_returns):\n",
        "    \"\"\"Plot the learning curves for ADAM and SGD optimizers side by side.\"\"\"\n",
        "    def moving_average(x, w):\n",
        "        return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot ADAM learning curve.\n",
        "    smoothed_adam = moving_average(adam_returns, 30)\n",
        "    axes[0].plot(smoothed_adam)\n",
        "    axes[0].set_title('ADAM Optimizer')\n",
        "    axes[0].set_xlabel('Number of Episodes')\n",
        "    axes[0].set_ylabel('Average Episode Returns')\n",
        "    axes[0].spines['right'].set_visible(False)\n",
        "    axes[0].spines['top'].set_visible(False)\n",
        "    axes[0].xaxis.set_ticks_position('bottom')\n",
        "    axes[0].yaxis.set_ticks_position('left')\n",
        "\n",
        "    # Plot SGD learning curve.\n",
        "    smoothed_sgd = moving_average(sgd_returns, 30)\n",
        "    axes[1].plot(smoothed_sgd)\n",
        "    axes[1].set_title('SGD Optimizer')\n",
        "    axes[1].set_xlabel('Number of Episodes')\n",
        "    axes[1].set_ylabel('Average Episode Returns')\n",
        "    axes[1].spines['right'].set_visible(False)\n",
        "    axes[1].spines['top'].set_visible(False)\n",
        "    axes[1].xaxis.set_ticks_position('bottom')\n",
        "    axes[1].yaxis.set_ticks_position('left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the learning curves.\n",
        "plot_learning_curve_custom(adam_returns, sgd_returns)"
      ],
      "metadata": {
        "id": "eLHhsgWMUcbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g80tBzfsBgya"
      },
      "source": [
        "[2pts]: What is the best asymptotic average return that can be achieved by the actor-critic agent described in `Part A)`? Can this be further improved, and if so: how? Explain your answer.\n",
        "\n",
        ">  As time t $\\rightarrow \\infty$, the best asymptotic average return is 1, which corresponds to following the optimal policy in the Catch environment. In our experiments, the agent approaches this maximum reward (1) around 1700 episodes, indicating that there is limited room for improvement in terms of achievable return. However, improvements can still be made by fine-tuning the hyperparameters. For instance, lowering the learning rate and incorporating techniques such as Generalized Advantage Estimation (GAE) can further reduce the variance in the policy gradient updates, leading to more stable and efficient learning.\n",
        "\n",
        "\n",
        "[2pts] What is the best asymptotic average return that can be achieved by the second agent described in `Part B)`? Can this be further improved, and if so: how? Explain your answer.\n",
        "\n",
        "> The second agent follows an $\\epsilon$-greedy policy. In our setup, the agent selects the greedy (optimal) action with probability 0.9 and a random action with probability 0.1. In the best-case scenario, when the greedy action always returns 1 and the random choice is uniformly over 3 actions, the average reward becomes:\n",
        "$$\n",
        "0.9 × 1 + 0.1 × (\\frac{1}{3} × 1 + \\frac{2}{3} \\times -1) \\approx 0.867\n",
        "$$\n",
        "This performance can be improved by reducing ε over time (i.e., annealing ε), so that the policy becomes more greedy as learning progresses. This would allow the agent to eventually approach an asymptotic return closer to 1 once it has sufficiently explored the environment.\n",
        "\n",
        "[2pts] What quantity do the preferences `p` estimate in the second agent described in Part B?\n",
        "\n",
        "> The preferences `p` are trained to approximate the action-value function, $q(s,a)$. In other words, they represent an estimate of the expected return obtained by taking action $a$ in state $s$ and then following the policy thereafter. This is reflected in the update rule:\n",
        "\n",
        "$$\n",
        "\\Delta w_p = \\alpha \\Bigl( R_{t+1} + \\gamma\\,v(S_{t+1}) - p(S_t, A_t) \\Bigr) \\nabla p(S_t, A_t)\n",
        "$$\n",
        "\n",
        "which is analogous to a TD update for $q(s,a)$. Thus, in the second agent, the vector of preferences `p` effectively estimates the action-values.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}