{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RL coursework, part II (25 pts total)\n",
    "---\n",
    "\n",
    "**Name:** Yuan Lu\n",
    "\n",
    "**SN:** 20114649\n",
    "\n",
    "---\n",
    "\n",
    "**Due date:** *April 10th, 2025*\n",
    "\n",
    "---\n",
    "\n",
    "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
    "\n",
    "## How to submit\n",
    "\n",
    "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part2.ipynb`** before the deadline above, where `<studentnumber>` is your student number.\n",
    "\n",
    "----\n",
    "**Reminder of copyrights**\n",
    "\n",
    "Copyrights protect this code/content and distribution or usages of it (or parts of it) without permission is prohibited. This includes uploading it and usage of it in training in any LLMs systems."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Context\n",
    "\n",
    "In this part, we will take a first look at learning algorithms for sequential decision problems.\n",
    "\n",
    "## Background reading\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 3 - 6"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Assignment\n",
    "\n",
    "### Objectives\n",
    "\n",
    "You will use Python to implement several reinforcement learning algorithms.\n",
    "\n",
    "You will then run these algorithms on a few problems, to understand their properties."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Useful Libraries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set options"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.set_printoptions(precision=3, suppress=1)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Some grid world"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "W = -100  # wall\n",
    "G = 100  # goal\n",
    "\n",
    "GRID_LAYOUT = np.array([\n",
    "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
    "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
    "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
    "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
    "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
    "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
    "])\n",
    "\n",
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, noisy=False):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = GRID_LAYOUT\n",
    "    self._start_state = (2, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._noisy = noisy\n",
    "\n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "    return self._number_of_states\n",
    "\n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return y*self._layout.shape[1] + x\n",
    "\n",
    "  def obs_to_state(self, obs):\n",
    "    x = obs % self._layout.shape[1]\n",
    "    y = obs // self._layout.shape[1]\n",
    "    s = np.copy(grid._layout)\n",
    "    s[y, x] = 4\n",
    "    return s\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "\n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    reward = self._layout[new_y, new_x]\n",
    "    if self._layout[new_y, new_x] == W:  # wall\n",
    "      discount = 0.9\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
    "      reward = -1.\n",
    "      discount = 0.9\n",
    "    else:  # a goal\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "\n",
    "    if self._noisy:\n",
    "      width = self._layout.shape[1]\n",
    "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
    "\n",
    "    self._state = new_state\n",
    "    return reward, discount, self.get_obs()\n",
    "\n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
    "    plt.gca().grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper functions\n",
    "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_experiment(env, agent, number_of_steps):\n",
    "  mean_reward = 0.\n",
    "  try:\n",
    "    action = agent.initial_action()\n",
    "  except AttributeError:\n",
    "    action = 0\n",
    "  for _ in range(number_of_steps):\n",
    "    reward, discount, next_state = env.step(action)\n",
    "    action = agent.step(reward, discount, next_state)\n",
    "    mean_reward += reward\n",
    "  return mean_reward/float(number_of_steps)\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
    "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
    "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "\n",
    "  plt.subplot(4, 3, 5)\n",
    "  v = np.max(q, axis=-1)\n",
    "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "  # Plot arrows:\n",
    "  plt.subplot(4, 3, 11)\n",
    "  plot_values(grid, grid==0, vmax=1)\n",
    "  for row in range(len(grid)):\n",
    "    for col in range(len(grid[0])):\n",
    "      if grid[row][col] == 0:\n",
    "        argmax_a = np.argmax(q[row, col])\n",
    "        if argmax_a == 0:\n",
    "          x = col\n",
    "          y = row + 0.5\n",
    "          dx = 0\n",
    "          dy = -0.8\n",
    "        if argmax_a == 1:\n",
    "          x = col - 0.5\n",
    "          y = row\n",
    "          dx = 0.8\n",
    "          dy = 0\n",
    "        if argmax_a == 2:\n",
    "          x = col\n",
    "          y = row - 0.5\n",
    "          dx = 0\n",
    "          dy = 0.8\n",
    "        if argmax_a == 3:\n",
    "          x = col + 0.5\n",
    "          y = row\n",
    "          dx = -0.8\n",
    "          dy = 0\n",
    "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
    "\n",
    "def plot_rewards(xs, rewards, color):\n",
    "  mean = np.mean(rewards, axis=0)\n",
    "  p90 = np.percentile(rewards, 90, axis=0)\n",
    "  p10 = np.percentile(rewards, 10, axis=0)\n",
    "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
    "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
    "\n",
    "def parameter_study(parameter_values, parameter_name, agent_constructor,\n",
    "                    env_constructor, color,\n",
    "                    repetitions=10, number_of_steps=int(1e4)):\n",
    "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
    "  for rep in range(repetitions):\n",
    "    for i, p in enumerate(parameter_values):\n",
    "      env = env_constructor()\n",
    "      agent = agent_constructor()\n",
    "      if 'eps' in parameter_name:\n",
    "        agent.set_epsilon(p)\n",
    "      elif 'alpha' in parameter_name:\n",
    "        agent._step_size = p\n",
    "      else:\n",
    "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
    "      mean_rewards[rep, i] = run_experiment(env, agent, number_of_steps)\n",
    "      agent.set_epsilon(0.)\n",
    "      agent._step_size = 0.\n",
    "      greedy_rewards[rep, i] = run_experiment(env, agent, number_of_steps//10)\n",
    "      del env\n",
    "      del agent\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_rewards(parameter_values, mean_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_rewards(parameter_values, greedy_rewards, color)\n",
    "  plt.yticks=([0, 1], [0, 1])\n",
    "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
    "  plt.xlabel(parameter_name, size=12)\n",
    "\n",
    "def epsilon_greedy(q_values, epsilon):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])\n",
    "\n",
    "\n",
    "def colorline(x, y, z):\n",
    "  \"\"\"\n",
    "  Based on:\n",
    "  http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n",
    "  http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n",
    "  Plot a colored line with coordinates x and y\n",
    "  Optionally specify colors in the array z\n",
    "  Optionally specify a colormap, a norm function and a line width\n",
    "  \"\"\"\n",
    "  segments = make_segments(x, y)\n",
    "  lc = mcoll.LineCollection(segments, array=z, cmap=plt.get_cmap('copper_r'),\n",
    "                            norm=plt.Normalize(0.0, 1.0), linewidth=3)\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.add_collection(lc)\n",
    "  return lc\n",
    "\n",
    "\n",
    "def make_segments(x, y):\n",
    "  \"\"\"\n",
    "  Create list of line segments from x and y coordinates, in the correct format\n",
    "  for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
    "  and y) array\n",
    "  \"\"\"\n",
    "  points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "  segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "  return segments\n",
    "\n",
    "\n",
    "def plotting_helper_function(_x, _y, title=None, ylabel=None):\n",
    "  z = np.linspace(0, 0.9, len(_x))**0.7\n",
    "  colorline(_x, _y, z)\n",
    "  plt.plot(0, 0, '*', color='#000000', ms=20, alpha=0.7, label='$w^*$')\n",
    "  plt.plot(1, 1, '.', color='#ee0000', alpha=0.7, ms=20, label='$w_0$')\n",
    "  min_y, max_y = np.min(_y), np.max(_y)\n",
    "  min_x, max_x = np.min(_x), np.max(_x)\n",
    "  min_y, max_y = np.min([0, min_y]), np.max([0, max_y])\n",
    "  min_x, max_x = np.min([0, min_x]), np.max([0, max_x])\n",
    "  range_y = max_y - min_y\n",
    "  range_x = max_x - min_x\n",
    "  max_range = np.max([range_y, range_x])\n",
    "  plt.arrow(_x[-3], _y[-3], _x[-1] - _x[-3], _y[-1] - _y[-3], color='k',\n",
    "            head_width=0.04*max_range, head_length=0.04*max_range,\n",
    "            head_starts_at_zero=False)\n",
    "  plt.ylim(min_y - 0.2*range_y, max_y + 0.2*range_y)\n",
    "  plt.xlim(min_x - 0.2*range_x, max_x + 0.2*range_x)\n",
    "  ax = plt.gca()\n",
    "  ax.ticklabel_format(style='plain', useMathText=True)\n",
    "  plt.legend(loc=2)\n",
    "  plt.xticks(rotation=12, fontsize=10)\n",
    "  plt.yticks(rotation=12, fontsize=10)\n",
    "  plt.locator_params(nbins=3)\n",
    "  if title is not None:\n",
    "    plt.title(title, fontsize=20)\n",
    "  if ylabel is not None:\n",
    "    plt.ylabel(ylabel, fontsize=20)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Section A: Tabular RL\n",
    "\n",
    "In this section, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.  You will implement agents, which should be in pure Python - so you cannot use JAX/TensorFlow/PyTorch to compute gradients. Using `numpy` is fine."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A random agent\n",
    "\n",
    "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Random(object):\n",
    "\n",
    "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
    "    self._number_of_actions = number_of_actions\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    next_action = np.random.randint(self._number_of_actions)\n",
    "    return next_action"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The grid\n",
    "\n",
    "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "GRID = Grid()\n",
    "GRID.plot_grid()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Q1: Implement TD learning **[4 pts]**\n",
    "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
    "\n",
    "Use the `__init__` as provided below.  You should implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.\n",
    "\n",
    "Also implement a property `state_values(self)` returning the vector of all state values (one value per state). (A method with the `@property` decorator can be called without the parentheses as if it's a variable, e.g., `agent.state_values` instead of `agent.state_values()`.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RandomTD(object):\n",
    "\n",
    "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
    "      self._number_of_states = number_of_states\n",
    "      self._number_of_actions = number_of_actions\n",
    "      self._step_size = step_size\n",
    "      # Initialize all state values to 0.\n",
    "      self._values = np.zeros(number_of_states)\n",
    "      # Store the current state; used to update its value upon receiving a transition.\n",
    "      self._current_state = initial_state\n",
    "\n",
    "  @property\n",
    "  def state_values(self):\n",
    "    return self._values\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    # One-step TD update for the current state's value:\n",
    "    # v(s) <- v(s) + alpha * (reward + discount * v(s') - v(s))\n",
    "    td_target = reward + discount * self._values[next_state]\n",
    "    td_error = td_target - self._values[self._current_state]\n",
    "    self._values[self._current_state] += self._step_size * td_error\n",
    "\n",
    "    # Update the current state to the new state\n",
    "    self._current_state = next_state\n",
    "\n",
    "    # Choose a random action (as the policy is random).\n",
    "    action = np.random.randint(self._number_of_actions)\n",
    "    return action"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
    "\n",
    "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero. The plotting code knows about the walls, so they appear black below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Do not modify this cell.\n",
    "AGENT = RandomTD(GRID._layout.size, 4, GRID.get_obs())\n",
    "run_experiment(GRID, AGENT, int(1e5))\n",
    "v = AGENT.state_values\n",
    "plot_values(GRID_LAYOUT,\n",
    "            v.reshape(GRID._layout.shape),\n",
    "            colormap=\"pink\", vmin=-400, vmax=100)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q2: Policy iteration **[5 pts]**\n",
    "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
    "\n",
    "2.1 **[3pts]** The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
    "\n",
    "> Yes, in a finite MDP, repeatedly evaluating the policy and then choosing the greedy policy with respect to its value function (i.e., policy iteration) will converge to the optimal policy.\n",
    "By the policy improvement theorem, each greedy update produces a policy that is at least as good as the current one and strictly better unless the current policy is already optimal.\n",
    "Because there are only finitely many possible policies, this improvement process must terminate in a finite number of steps with the optimal policy.\n",
    "\n",
    "2.2. **[2pts]** What if you were to run a Q-learning agent on this enveromenint, and consider running to converegence, how would the value function for Q_learning compare to the one learnt by the TD agent?\n",
    "\n",
    "> When run to convergence, Q‑learning learns the unique fixed point of the Bellman **optimality** operator, i.e. the optimal action‑value function $Q^*(s,a)$. Converting this to state values via $v^*(s)=\\max_a Q^*(s,a)$ yields the optimal state‑value function, whereas the TD agent described here only performs policy evaluation for a fixed random policy and thus converges to $v_{\\pi_{\\text{random}}}(s)$, the value of that suboptimal policy. Consequently, for every state $s$,\n",
    "$$\n",
    "v^*(s) \\;\\ge\\; v_{\\pi_{\\text{random}}}(s),\n",
    "$$\n",
    "with strict inequality wherever the random policy is suboptimal, reflecting that Q‑learning discovers the best possible value while the TD agent merely estimates the value of its current (random) policy.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Section B: Off-policy Bellman operators with function approximation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q1: Bellman operator for prediction **[5 pts]**\n",
    "\n",
    "We are going to implement the _expected_ update to the weight for a simple MDP.\n",
    "In order to be able to implement this update, we need to know the MDP, as well as the way the agent's value depend on the weights and states.  This is defined as follows:\n",
    "\n",
    "There are two states, $s_1$ and $s_2$.  All rewards are zero, and therefore can be ignored.  Instead of the underlying state, the agent observes state features $\\mathbf{x}_1 = \\mathbf{x}(s_1)$ and $\\mathbf{x}_2 = \\mathbf{x}(s_2)$.  These are defined for the two states are $\\mathbf{x}_1 = [1, 1]^{\\top}$ and $\\mathbf{x}_2 = [2, 1]^{\\top}$.  In each state, there are two actions, $a$ and $b$.  Action $a$ always transitions to state $s_1$, action $b$ always transitions to state $s_2$.\n",
    "\n",
    "![MDP](https://hadovanhasselt.files.wordpress.com/2020/02/mdp.png)\n",
    "\n",
    "---\n",
    "\n",
    "The agent will make *linear* predicions, such that $v(s) = \\mathbf{w}^\\top \\mathbf{x}(s)$. In the code cell below, you should implement an update that computes the **expected** weight update, given:\n",
    "* The current weights `w`.\n",
    "* A target policy $\\pi$ that we want to learn about; the actual input, denoted `pi` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `pi` $= \\pi(a|s), \\forall s$.\n",
    "* A behaviour policy $\\mu$ that would be used to generate the transitions; the actual input, denoted `mu` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `mu` $= \\mu(a|s), \\forall s$.\n",
    "* A scalar trace parameter $\\lambda$ (=`trace_parameter`),\n",
    "* A scalar discount parameter $\\gamma$ (=`discount`).\n",
    "\n",
    "The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state.  The step size of the update should be $\\alpha=0.1$.\n",
    "\n",
    "The expected update should be for a multi-step $\\lambda$-return and should be correct for any pair of target and any behaviour policy.  It should be the expectation of performing a (forward view) TD($\\lambda$) update in the MDP described above when the _state distribution_ is generated by the behaviour policy and the _returns_ from each state are generated by the target policy. We will use the update you implement to generate plots below.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# state features (do not change)\n",
    "x1 = np.array([1., 1.])\n",
    "x2 = np.array([2., 1.])\n",
    "\n",
    "def expected_update(w, pi, mu, trace_parameter, discount, lr):\n",
    "  alpha = lr\n",
    "\n",
    "  # Compute current value estimates:\n",
    "  v1 = np.dot(w, x1)\n",
    "  v2 = np.dot(w, x2)\n",
    "\n",
    "  # Under the target policy (pi for action a leads to s1, 1-pi for action b leads to s2),\n",
    "  # the expected value for the next state is:\n",
    "  v_bar = pi * v1 + (1 - pi) * v2\n",
    "\n",
    "  # Compute the multi-step λ-return (forward view) in our reward-free MDP:\n",
    "  G_lambda = discount * (1 - trace_parameter) / (1 - trace_parameter * discount) * v_bar\n",
    "\n",
    "  # TD errors for each state:\n",
    "  td_error1 = G_lambda - v1\n",
    "  td_error2 = G_lambda - v2\n",
    "\n",
    "  # The behaviour policy gives state s1 with probability mu, and s2 with probability (1-mu).\n",
    "  expected_weight_update = mu * td_error1 * x1 + (1 - mu) * td_error2 * x2\n",
    "\n",
    "  return alpha * expected_weight_update\n",
    "  #return expected_weight_update"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment: run the cell below\n",
    "The cell below runs an experiment, across different target policies and trace parameters $\\lambda$.\n",
    "\n",
    "The plots below the cell will show how the weights move within the 2-dimensional weight space, starting from $w_0 = [1, 1]^{\\top}$ (shown as red dot).  The optimal solution $w_* = [0, 0]^{\\top}$ is also shown (as black star)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_ws(w, pi, mu, l, g):\n",
    "  \"\"\"Apply the expected update 1000 times\"\"\"\n",
    "  ws = [w]\n",
    "  for _ in range(1000):\n",
    "    w = w + expected_update(w, pi, mu, l, g, lr=0.1)\n",
    "    ws.append(w)\n",
    "  return np.array(ws)\n",
    "\n",
    "mu = 0.5  # behaviour\n",
    "g = 0.99  # discount\n",
    "\n",
    "lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])\n",
    "pis = np.array([0., 0.1, 0.2, 0.5, 1.])\n",
    "\n",
    "fig = plt.figure(figsize=(22, 17))\n",
    "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
    "\n",
    "for r, pi in enumerate(pis):\n",
    "  for c, l in enumerate(lambdas):\n",
    "    plt.subplot(len(pis), len(lambdas), r*len(lambdas) + c + 1)\n",
    "    w = np.ones_like(x1)\n",
    "    ws = generate_ws(w, pi, mu, l, g)\n",
    "    title = '$\\\\lambda={:1.3f}$'.format(l) if r == 0 else None\n",
    "    ylabel = '$\\\\pi={:1.1f}$'.format(pi) if c == 0 else None\n",
    "    plotting_helper_function(ws[:, 0], ws[:, 1], title, ylabel)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q2: Analyse results (11 pts total)\n",
    "1. **[1 pts]** How many of the above 25 experiments diverge?\n",
    "1. **[1 pts]** For which policies $\\pi$, is the true value function $v_{\\pi}$ representable in the above feature space (spanned by $x_1, x_2$).\n",
    "1. **[2 pts]** Why are the results asymmetric across different $\\pi$?  In particular, explain why the results look different when comparing $\\pi = \\pi(a | \\cdot) = 0$ to $\\pi(a | \\cdot) = 1$.\n",
    "1. **[2 pts]** For which combination of $\\pi(a)$ and $\\lambda$ does the expected update (with uniform random behaviour) converge? Do not limit the answer to the subset of values in the plots above, but to all possible choices of $\\lambda \\in [0, 1]$ and $\\pi(a|s) \\in [0, 1]$, but do restrict yourself to state-less policies, as above, for which the action probabilities are equal in the two states: $\\pi(a|s_1) = \\pi(a|s_2)$.\n",
    "1. **[1 pts]** Why do all the plots corresponding to full Monte Carlo update look the same (right column)?\n",
    "1. **[2 pts]** Why do the plots corresponding to full Monte Carlo have the shape they do?\n",
    "1. **[2 pts]** The plots above are for uniform behavoiur: $\\mu(a|s) = \\mu(b|s) = 0.5$.  How would the results change (high level, not in terms of precise plots) if the behaviour policy $\\mu$ would select action $a$ more often (e.g., $\\mu = 0.8$)?  How would the results change if the behaviour would select $a$ less often (e.g., $\\mu = 0.2$)?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Put answers to Q8 in this cell:\n",
    "1. **7** divergence:\n",
    "   - (π=0, λ∈{0,0.8,0.9}),\n",
    "   - (π=0.1, λ∈{0,0.8}),\n",
    "   - (π=0.2, λ∈{0,0.8}).\n",
    "---\n",
    "2. All policies $pi$ yield a true value function $v_\\pi(s_1) = 0$and $v_\\pi(s_2) = 0$, because the MDP is reward-free and transitions forever. This zero function is trivially in the span of the two feature vectors $\\mathbf{x}_1=[1,1]^\\top$ and $\\mathbf{x}_2=[2,1]^\\top$. Setting $\\mathbf{w}=\\mathbf{0}$ yields $v(s) = 0$ for both states, matching $v_\\pi$. Hence, **every** policy’s value function is representable by some weight vector (specifically $\\mathbf{w}=\\mathbf{0}$).\n",
    "---\n",
    "3. The agent’s update depends on how often the target policy transitions to $s_1$ versus $s_2$. Because $\\mathbf{x}_1\\neq \\mathbf{x}_2$ and the magnitude of $\\mathbf{x}_2$ is larger in the first coordinate, the learning dynamics differ depending on whether $\\pi\\approx 0$ (mostly taking the action leading to $s_2$) versus $\\pi\\approx 1$ (mostly taking the action leading to $s_1$). In other words, the update target involves $\\pi\\,v_1+(1-\\pi)\\,v_2$, which shifts the update direction; thus, $\\pi=0$ and $\\pi=1$ lead to different “pulls” in weight space. To be more detailed one can derive:\n",
    "\n",
    "The one‑step lookahead (target) value under policy π is\n",
    "$$\n",
    "\\bar v \\;=\\;\\pi\\,v_1 + (1-\\pi)\\,v_2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_1 = w^\\top x_1 = w_1 + w_2,\\quad\n",
    "v_2 = w^\\top x_2 = 2w_1 + w_2.\n",
    "$$\n",
    "\n",
    "Because all rewards are zero, the forward‑view TD(λ) target simplifies to\n",
    "$$\n",
    "G_\\lambda \\;=\\;\\frac{\\gamma(1-\\lambda)}{1-\\gamma\\lambda}\\,\\bar v \\;=\\;c(\\lambda)\\,\\bigl((2-\\pi)w_1 + w_2\\bigr),\n",
    "$$\n",
    "with $c(\\lambda)=\\frac{\\gamma(1-\\lambda)}{1-\\gamma\\lambda}$.\n",
    "\n",
    "The expected weight update under behaviour μ is\n",
    "$$\n",
    "\\mathbb{E}[\\Delta w]\n",
    "=\\alpha\\Bigl[\\mu\\bigl(G_\\lambda - v_1\\bigr)x_1\n",
    "+(1-\\mu)\\bigl(G_\\lambda - v_2\\bigr)x_2\\Bigr].\n",
    "$$\n",
    "\n",
    "Substitute $x_1=[1,1]^\\top$, $x_2=[2,1]^\\top$, $v_1=w_1+w_2$, $v_2=2w_1+w_2$:\n",
    "$$\n",
    "\\mathbb{E}[\\Delta w]\n",
    "=\\alpha\\begin{bmatrix}\n",
    "\\mu\\bigl(c((2-\\pi)w_1+w_2)-(w_1+w_2)\\bigr)\n",
    "+2(1-\\mu)\\bigl(c((2-\\pi)w_1+w_2)-(2w_1+w_2)\\bigr)\\\\[6pt]\n",
    "\\mu\\bigl(c((2-\\pi)w_1+w_2)-(w_1+w_2)\\bigr)\n",
    "+(1-\\mu)\\bigl(c((2-\\pi)w_1+w_2)-(2w_1+w_2)\\bigr)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    " Compare π=0 versus π=1\n",
    "\n",
    "1. **π=0** ⇒ $G_\\lambda=c(2w_1+w_2)$.\n",
    "   $$\n",
    "   \\mathbb{E}[\\Delta w]_{\\pi=0}\n",
    "   =\\alpha\\begin{bmatrix}\n",
    "   \\mu\\bigl((2c-1)w_1+(c-1)w_2\\bigr)\n",
    "   +2(1-\\mu)\\bigl((2c-2)w_1+(c-1)w_2\\bigr)\\\\[4pt]\n",
    "   \\mu\\bigl((2c-1)w_1+(c-1)w_2\\bigr)\n",
    "   +(1-\\mu)\\bigl((2c-2)w_1+(c-1)w_2\\bigr)\n",
    "   \\end{bmatrix}.\n",
    "   $$\n",
    "\n",
    "2. **π=1** ⇒ $G_\\lambda=c(w_1+w_2)$.\n",
    "   $$\n",
    "   \\mathbb{E}[\\Delta w]_{\\pi=1}\n",
    "   =\\alpha\\begin{bmatrix}\n",
    "   \\mu\\bigl((c-1)(w_1+w_2)\\bigr)\n",
    "   +2(1-\\mu)\\bigl((c-2)w_1+(c-1)w_2\\bigr)\\\\[4pt]\n",
    "   \\mu\\bigl((c-1)(w_1+w_2)\\bigr)\n",
    "   +(1-\\mu)\\bigl((c-2)w_1+(c-1)w_2\\bigr)\n",
    "   \\end{bmatrix}.\n",
    "   $$\n",
    "\n",
    "Since these two update vectors are **not scalar multiples** of each other (due to different coefficients on $w_1$ and $w_2$), the direction of the expected weight update depends on π, proving the observed asymmetry in the plots.\n",
    "\n",
    "---\n",
    "4. In linear function approximation with a forward-view TD($\\lambda$) update, convergence occurs if the effective update matrix has eigenvalues of magnitude less than 1. For our 2-state/2-feature setup, one can analyze the corresponding 2×2 system matrix. A simplified criterion  is that the scalar $\\kappa(\\lambda) = \\frac{\\gamma(1-\\lambda)}{1-\\gamma\\lambda}$ and the scaling induced by $\\pi$ should keep the update matrix stable. Concretely, for $\\lambda<1$ and $\\gamma<1$, $\\kappa<\\infty$, but if $\\kappa$ and $\\pi$ yield certain combinations that push the updates too aggressively in weight space, divergence can occur. In practice, it often diverges when $\\pi$ is near 0 (heavy use of $\\mathbf{x}_2$) and $\\lambda$ is high.\n",
    "\n",
    "Proof:\n",
    "Starting from the expected TD(λ) weight update (with μ=0.5):\n",
    "\n",
    "$$\n",
    "\\Delta w\n",
    "= \\frac{\\alpha}{2}\\bigl[(G_\\lambda - v_1)x_1 + (G_\\lambda - v_2)x_2\\bigr],\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G_\\lambda\n",
    "= c(\\lambda)\\bigl(\\pi v_1 + (1-\\pi)v_2\\bigr),\n",
    "\\quad\n",
    "c(\\lambda)=\\frac{\\gamma(1-\\lambda)}{1-\\gamma\\lambda}.\n",
    "$$\n",
    "\n",
    "Writing this as a linear map \\(\\Delta w = \\tfrac{\\alpha}{2}X\\,w\\), one finds\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "3c(\\,2-\\pi\\,)-5 & 3c-3\\\\[4pt]\n",
    "2c(\\,2-\\pi\\,)-3 & 2c-2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Thus the one‑step update is\n",
    "$$\n",
    "w_{t+1} = w_t + \\frac{\\alpha}{2}X\\,w_t\n",
    "\\quad\\Longrightarrow\\quad\n",
    "w_{t+1} = A\\,w_t,\\quad\n",
    "A = I + \\frac{\\alpha}{2}X.\n",
    "$$\n",
    "\n",
    "Stability via Jury Conditions\n",
    "\n",
    "For both eigenvalues of \\(A\\) to lie strictly inside the unit circle, the following must hold:\n",
    "\n",
    "- i $1 - \\mathrm{tr}(A) + \\det(A) > 0$\n",
    "- ii $1 + \\mathrm{tr}(A) + \\det(A) > 0$\n",
    "- iii $1 - \\det(A) > 0$\n",
    "\n",
    "With $\\alpha=0.1$, direct calculation yields\n",
    "\n",
    "- $\\det(A)>0$ and condition(i) reduce to $c(\\lambda)<1$, always true for $\\lambda<1,\\gamma<1$.\n",
    "- Condition(iii) is also always satisfied.\n",
    "- Condition(ii) simplifies exactly to\n",
    "\n",
    "$$\n",
    "\\kappa(3\\pi - 8) + 7 > 0,\n",
    "\\quad\n",
    "\\kappa = \\frac{\\gamma(1-\\lambda)}{1-\\gamma\\lambda}.\n",
    "$$\n",
    "\n",
    "Rewriting gives the final convergence criterion:\n",
    "\n",
    "$$\n",
    "3\\lambda\\pi - 3\\pi - \\lambda \\;<\\; \\frac{7}{\\gamma} - 8.\n",
    "$$\n",
    "Whenever this inequality holds, the spectral radius of \\(A\\) is <1 and the expected update converges; otherwise it diverges.\n",
    "\n",
    "\n",
    "---\n",
    "5. In the reward-free case, the return $G_t$ is always zero, so the Monte Carlo update:\n",
    "$$\n",
    "\\mathbb{E}[\\Delta w]\n",
    "=\\alpha\\Bigl[\\mu\\bigl(G_\\lambda - v_1\\bigr)x_1\n",
    "+(1-\\mu)\\bigl(G_\\lambda - v_2\\bigr)x_2\\Bigr].\n",
    "$$\n",
    "reduces to:\n",
    "$$\n",
    "\\Delta \\mathbf{w}\\propto -v(s)\\mathbf{x}(s)\n",
    "$$\n",
    "This update does not depend on $\\pi$ at all (no 1-step lookahead, etc.). Consequently, all runs with $\\lambda=1$ have identical updates—hence the same trajectories in weight space—regardless of $\\pi$.\n",
    "---\n",
    "6. With $\\lambda=1$, the update is $\\Delta \\mathbf{w}\\approx -\\alpha \\bigl[\\mu\\,v(s_1)\\,\\mathbf{x}_1 + (1-\\mu)\\,v(s_2)\\,\\mathbf{x}_2\\bigr]$. Because $\\mathbf{x}_2$ has a larger first component than $\\mathbf{x}_1$, there is a stronger pull on $w_1$ initially, causing $w_1$ to drop quickly. Once $w_1$ gets small or negative, the ratio of updates shifts, creating a characteristic bend in the trajectory as $w_2$ continues to adjust.\n",
    "---\n",
    "7. The expected update is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\mu[\\Delta w]\n",
    "= \\alpha\\Bigl[\\mu\\,x_1\\,(G_\\lambda - v_1) + (1-\\mu)\\,x_2\\,(G_\\lambda - v_2)\\Bigr].\n",
    "$$\n",
    "\n",
    "Because $x_2=[2,1]^\\top$ has a larger first‑coordinate than $x_1=[1,1]^\\top$, updates from state $s_2$ pull $w_1$ twice as strongly (in the negative direction when $G_\\lambda<v_2$) as updates from $s_1$. As $G_\\lambda$ grows, $(G_\\lambda-v_1)$ becomes positive before $(G_\\lambda-v_2)$, so the $s_1$ term starts pushing $w$ away from zero sooner.\n",
    "\n",
    "- **Higher $\\mu$ (e.g. 0.8)** weights the $s_1$ update more heavily, lowering the threshold at which the update becomes positive along $w_1$, thus accelerating divergence.\n",
    "- **Lower $\\mu$ (e.g. 0.2)** gives more weight to $s_2$, whose stronger negative pull on $w_1$ delays the onset of positive updates and keeps weights moving toward zero longer, making convergence more likely.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook3",
    "kind": "private"
   },
   "name": "UCL RL assignment 2025, part II",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2025/RL_assignment_2.ipynb",
     "timestamp": 1741344842732
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2024/RL_assignment_2.ipynb",
     "timestamp": 1741191305645
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_2_solutions.ipynb",
     "timestamp": 1645016484699
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
     "timestamp": 1582541397384
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
     "timestamp": 1581969444858
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
     "timestamp": 1581964637124
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
     "timestamp": 1581957222796
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb?workspaceId=mtthss:ucl::citc",
     "timestamp": 1581683857481
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb",
     "timestamp": 1581608852108
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
     "timestamp": 1580904796770
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
     "timestamp": 1548782473207
    },
    {
     "file_id": "1t1PIXqa3m-irLvQ_fQ7qgaBZeTNqiG2v",
     "timestamp": 1542801802962
    },
    {
     "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
     "timestamp": 1517862636703
    },
    {
     "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
     "timestamp": 1517660129183
    },
    {
     "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
     "timestamp": 1517174839485
    },
    {
     "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
     "timestamp": 1515086437469
    },
    {
     "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
     "timestamp": 1511098107887
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
